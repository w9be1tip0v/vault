---
title: "Byte Latent Transformer: Patches Scale Better Than Tokens"
source: "https://arxiv.org/html/2412.09871v1"
author: "Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srinivasan Iyer"
published: "2024-12-13"
created: 2025-06-03
description: "Byte Latent Transformer (BLT) is a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it."
tags:
  - type/literature
  - theme/research
  - theme/learning
  - keyword/machine-learning
  - keyword/transformer
  - keyword/language-model 
  - keyword/tokenization
  - keyword/byte-processing
  - keyword/scaling
  - keyword/ai
  - keyword/nlp
  - keyword/research-paper
---
arXiv:2412.09871v1 \[cs.CL\] 13 Dec 2024

\]FAIR at Meta 1\]Paul G. Allen School of Computer Science & Engineering, University of Washington 2\]University of Chicago

\[â€¡\]Joint second author\[â€ \]Joint last author\[â‹„\]Work done at Meta

Artidoro Pagnoni Ram Pasunuru Pedro Rodriguez John Nguyen Benjamin Muller Margaret Li Chunting Zhou Lili Yu Jason Weston Luke Zettlemoyer Gargi Ghosh Mike Lewis Ari Holtzman Srinivasan Iyer \[\[\[[cs.washington.edu](https://arxiv.org/html/) [meta.com](https://arxiv.org/html/)

(December 13, 2024)

###### Abstract

We introduce the Byte Latent Transformer (B LT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.B LT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, B LT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.

artidoro at, sviyer at

\[Code\] [https://github.com/facebookresearch/blt](https://github.com/facebookresearch/blt)

## 1 Introduction

![Refer to caption](https://arxiv.org/html/x1.png)

Figure 1: Scaling trends for fixed inference flop models (fully) trained with varying training budgets. In token-based models, a fixed inference budget determines the model size. In contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget. B LT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama 2 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.

![Refer to caption](https://arxiv.org/html/x3.png)

Figure 2: B LT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. LT incorporates byte n ğ‘› italic\_n -gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modulesÂ ( Figure 5 ). Unlike fixed-vocabulary tokenization, LT dynamically groups bytes into patches preserving access to the byte-level information.

We introduce the Byte Latent Transformer (BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (Â§ [6](https://arxiv.org/html/2412.09871v1#S6 "6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). Existing large language models (llm s) are trained almost entirely end-to-end, except for tokenizationâ€”a heuristic pre-processing step that groups bytes into a static set of tokens. Such tokens bias how a string is compressed, leading to shortcomings such as domain/modality sensitivity [^11], sensitivity to input noise (Â§ [6](https://arxiv.org/html/2412.09871v1#S6 "6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), a lack of orthographic knowledge [^14], and multilingual inequity [^29].

Tokenization has previously been essential because directly training llm s on bytes is prohibitively costly at scale due to long sequence lengths [^47]. Prior works mitigate this by employing more efficient self-attention [^15] or attention-free architectures [^45] (Â§ [8](https://arxiv.org/html/2412.09871v1#S8 "8 Related Work â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). However, this primarily helps train small models. At scale, the computational cost of a Transformer is dominated by large feed-forward network layers that run on every byte, not the cost of the attention mechanism.

To efficiently allocate compute, we propose a dynamic, learnable method for grouping bytes into patches Â (Â§ [2](https://arxiv.org/html/2412.09871v1#S2 "2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")) and a new model architecture that mixes byte and patch information. Unlike tokenization, B LT has no fixed vocabulary for patches. Arbitrary groups of bytes are mapped to latent patch representations via light-weight learned encoder and decoder modules. We show that this results in more efficient allocation of compute than tokenization-based models.

Tokenization-based llm s allocate the same amount of compute to every token. This trades efficiency for performance, since tokens are induced with compression heuristics that are not always correlated with the complexity of predictions. Central to our architecture is the idea that models should dynamically allocate compute where it is needed. For example, a large transformer is not needed to predict the ending of most words, since these are comparably easy, low-entropy decisions compared to choosing the first word of a new sentence. This is reflected in B LTâ€™s architectureÂ (Â§ [3](https://arxiv.org/html/2412.09871v1#S3 "3 BLT Architecture â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")) where there are three transformer blocks: two small byte-level local models and a large global latent transformer Â ([Figure 2](https://arxiv.org/html/2412.09871v1#S1.F2 "Figure 2 â€£ 1 Introduction â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). To determine how to group bytes into patches and therefore how to dynamically allocate compute, B LT segments data based on the entropy of the next-byte prediction creating contextualized groupings of bytes with relatively uniform information density.

We present the first flop -controlled scaling study of byte-level models up to 8B parameters and 4T training bytes, showing that we can train a model end-to-end at scale from bytes without fixed-vocabulary tokenization. Overall, B LT matches training flop -controlled performance <sup>1</sup> <sup>1</sup> 1 We calculate the computational cost of a model by counting the number of Floating Point OPerations (flop s) needed. of Llama 3 while using up to 50% fewer flop s at inferenceÂ (Â§ [5](https://arxiv.org/html/2412.09871v1#S5 "5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). We also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data. B LT models are more robust than tokenizer-based models to noisy inputs and display enhanced character level understanding abilities demonstrated on orthographic knowledge, phonology, and low-resource machine translation tasksÂ (Â§ [6](https://arxiv.org/html/2412.09871v1#S6 "6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). Finally, with B LT models, we can simultaneously increase model size and patch size while maintaining the same inference flop budget. Longer patch sizes, on average, save compute which can be reallocated to grow the size of the global latent transformer, because it is run less often. We conduct inference- flop controlled scaling experimentsÂ ([Figure 1](https://arxiv.org/html/2412.09871v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), and observe significantly better scaling trends than with tokenization-based architectures.

In summary, this paper makes the following contributions: 1) We introduce B LT, a byte latent llm architecture that dynamically allocates compute to improve flop efficiency, 2) We show that we achieve training flop -controlled parity with Llama 3 up to 8B scale while having the option to trade minor losses in evaluation metrics for flop efficiency gains of up to 50%, 3) B LT models unlock a new dimension for scaling llm s, where model size can now be scaled while maintaining a fixed-inference budget, 4) We demonstrate the improved robustness of B LT models to input noise and their awareness of sub-word aspects of input data that token-based llm s miss. We release the training and inference code for B LT atÂ  [https://github.com/facebookresearch/blt](https://github.com/facebookresearch/blt).

## 2 Patching: From Individual Bytes to Groups of Bytes

![Refer to caption](https://arxiv.org/html/extracted/6066458/assets/patching_types.png)

Figure 3: 48

Segmenting bytes into patches allows B LT to dynamically allocate compute based on context. FigureÂ  [3](https://arxiv.org/html/2412.09871v1#S2.F3 "Figure 3 â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") shows several different methods for segmenting bytes into patches. Formally, a patching function $f_{p}$ segments a sequence of bytes $\boldsymbol{x}=\{x_{i},|i=1,\ldots n\}$ of length $n$ into a sequence of $m<n$ patches $\boldsymbol{p}=\{p_{j}|j=1,\ldots,m\}$ by mapping each $x_{i}$ to the set {0,1} where 1 indicates the start of a new patch. For both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer. In B LT, this is the number of patches needed to encode the data with a given patching function. Consequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching functionÂ (Â§ [4.5](https://arxiv.org/html/2412.09871v1#S4.SS5 "4.5 FLOPs Estimation â€£ 4 Experimental Setup â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). Next, we introduce three patching functions: patching with a fixed number of bytes per patchÂ (Â§ [2.1](https://arxiv.org/html/2412.09871v1#S2.SS1 "2.1 Strided Patching Every K Bytes â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), whitespace patchingÂ (Â§ [2.2](https://arxiv.org/html/2412.09871v1#S2.SS2 "2.2 Space Patching â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), and dynamically patching with entropies from a small byte lm Â (Â§ [2.3](https://arxiv.org/html/2412.09871v1#S2.SS3 "2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). Finally, we discuss incremental patching and how tokenization is different from patchingÂ (Â§ [2.4](https://arxiv.org/html/2412.09871v1#S2.SS4 "2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")).

### 2.1 Strided Patching Every K Bytes

Perhaps the most straightforward way to group bytes is into patches of fixed size $k$ as done in MegaByte [^48]. The fixed stride is easy to implement for training and inference, provides a straightforward mechanism for changing the average patch size, and therefore makes it easy to control the flop cost. However, this patching function comes with significant downsides. First, compute is not dynamically allocated to where it is needed most: one could be either wasting a transformer step $j$ if only predicting whitespace in code, or not allocating sufficient compute for bytes dense with information such as math. Second, this leads to inconsistent and non-contextual patching of similar byte sequences, such as the same word being split differently.

### 2.2 Space Patching

[^39] proposes a simple yet effective improvement over strided patching that creates new patches after any space-like bytes <sup>2</sup> <sup>2</sup> 2 Space-like bytes are defined as any byte that is not a latin character, digit, or utf -8 continuation byte. In addition, each patch must contain at least one non space-like byte.which are natural boundaries for linguistic units in many languages. In Space patching, a latent transformer step (i.e., more flop s) is allocated to model every word. This ensures words are patched in the same way across sequences and that flops are allocated for hard predictions which often follow spaces. For example, predicting the first byte of the answer to the question â€œWho composed the Magic Flute?â€ is much harder than predicting the remaining bytes after â€œMâ€ since the first character significantly reduces the number of likely choices, making the completion â€œMozartâ€ comparatively easy to predict. However, space patching cannot gracefully handle all languages and domains, and most importantly cannot vary the patch size. Next, we introduce a new patching method that uses the insight that the first bytes in words are typically most difficult to predict, but that provides a natural mechanism for controlling patch size.

### 2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM

Rather than relying on a rule-based heuristic such as whitespace, we instead take a data-driven approach to identify high uncertainty next-byte predictions. We introduce entropy patching, which uses entropy estimates to derive patch boundaries.

We train a small byte-level auto-regressive language model on the training data for B LT and compute next byte entropies under the LM distribution $p_{e}$ over the byte vocabulary $\mathcal{V}$ :

<table><tbody><tr><td></td><td><math><semantics><mrow><mrow><mi>H</mi> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub><mo>)</mo></mrow></mrow> <mo>=</mo> <mrow><munder><mo>âˆ‘</mo> <mrow><mi>v</mi> <mo>âˆˆ</mo> <mi>ğ’±</mi></mrow></munder> <mrow><msub><mi>p</mi> <mi>e</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mi>v</mi> <mo>|</mo> <msub><mi>ğ’™</mi> <mrow><mo>&lt;</mo> <mi>i</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow> <mo>â¢</mo> <mrow><mi>log</mi> <mo>â¡</mo> <msub><mi>p</mi> <mi>e</mi></msub></mrow> <mo>â¢</mo> <mrow><mo>(</mo><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mi>v</mi> <mo>|</mo> <msub><mi>ğ’™</mi> <mrow><mo>&lt;</mo> <mi>i</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow> <annotation-xml><apply><apply><ci>ğ»</ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘¥</ci> <ci>ğ‘–</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <apply><ci>ğ‘£</ci> <ci>ğ’±</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘</ci> <ci>ğ‘’</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘¥</ci> <ci>ğ‘–</ci></apply> <apply><csymbol>conditional</csymbol> <ci>ğ‘£</ci> <apply><csymbol>subscript</csymbol> <ci>ğ’™</ci> <apply><csymbol>absent</csymbol> <ci>ğ‘–</ci></apply></apply></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘</ci> <ci>ğ‘’</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘¥</ci> <ci>ğ‘–</ci></apply> <apply><csymbol>conditional</csymbol> <ci>ğ‘£</ci> <apply><csymbol>subscript</csymbol> <ci>ğ’™</ci> <apply><csymbol>absent</csymbol> <ci>ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml> <annotation>H(x_{i})=\sum_{v\in\mathcal{V}}p_{e}(x_{i}=v|\boldsymbol{x}_{&lt;i})\log p_{e}(x_% {i}=v|\boldsymbol{x}_{&lt;i})</annotation> <annotation>italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = âˆ‘ start_POSTSUBSCRIPT italic_v âˆˆ caligraphic_V end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_v | bold_italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ) roman_log italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_v | bold_italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(1)</span></td></tr></tbody></table>

![Refer to caption](https://arxiv.org/html/x4.png)

Figure 4: This figure plots the entropy H â¢ ( x i ) ğ» subscript ğ‘¥ ğ‘– H(x\_{i}) italic\_H ( italic\_x start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) of each byte in â€œDaenerys Targeryen is in Game of Thrones, a fantasy epic by George R.R. Martin.â€ with spaces shown as underscores. Patches end when exceeds the global threshold Î¸ g ğœƒ ğ‘” \\theta\_{g} italic\_Î¸ start\_POSTSUBSCRIPT italic\_g end\_POSTSUBSCRIPT, shown as a red horizontal line. The start of new patches are shown with vertical gray lines. For example, the entropies of â€œGâ€ and â€œeâ€ in â€œGeorge R.R. Martinâ€ exceed, so â€œGâ€ is the start of a single byte patch and â€œeâ€ of a larger patch extending to the end of the named entity as the entropy stays low, resulting in no additional patches.

We experiment with two methods to identify patch boundaries given entropies $H(x_{i})$ . The first, finds points above a global entropy threshold, as illustrated in [Figure 4](https://arxiv.org/html/2412.09871v1#S2.F4 "Figure 4 â€£ 2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). The second, identifies points that are high relative to the previous entropy. The second approach can also be interpreted as identifying points that break approximate monotonically decreasing entropy withing the patch.

|  | $\displaystyle\text{Global Constraint}\hskip 28.45274ptH(x_{t})$ | $\displaystyle>\theta_{g}$ |  |
| --- | --- | --- | --- |
|  | $\displaystyle\text{Approx. Monotonic Constraint}\hskip 28.45274ptH(x_{t})$ | $\displaystyle-H(x_{t-1})>\theta_{r}$ |  |

Patch boundaries are identified during a lightweight preprocessing step executed during dataloading. This is different from [^34] where classifier is trained to predict entropy-based patch boundaries. In our experiments (Â§ [4](https://arxiv.org/html/2412.09871v1#S4 "4 Experimental Setup â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), we compare these two methods for distinguishing between low and high entropy bytes.

### 2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching

Many modern llm s, including our baseline Llama 3, use a subword tokenizer like bpe [^16]. We use â€œtokensâ€ to refer to byte-groups drawn from a finite vocabulary determined prior to training as opposed to â€œpatchesâ€ which refer to dynamically grouped sequences without a fixed vocabulary. A critical difference between patches and tokens is that with tokens, the model has no direct access to the underlying byte features.

A crucial improvement of B LT over tokenization-based models is that redefines the trade off between the vocabulary size and compute. In standard llm s, increasing the size of the vocabulary means larger tokens on average and therefore fewer steps for the model but also larger output dimension for the final projection layer of the model. This trade off effectively leaves little room for tokenization based approaches to achieve significant variations in token size and inference cost. For example, Llama 3 increases the average token size from 3.7 to 4.4 bytes at the cost of increasing the size of its embedding table 4x compared to Llama 2.

When generating, B LT needs to decide whether the current step in the byte sequence is at a patch boundary or not as this determines whether more compute is invoked via the Latent Transformer. This decision needs to occur independently of the rest of the sequence which has yet to be generated. Thus patching cannot assume access to future bytes in order to choose how to segment the byte sequence. Formally, a patching scheme $f_{p}$ satisfies the property of incremental patching if it satisfies:

|  | $$ f_{p}(\boldsymbol{x}_{<i})=f_{p}(\boldsymbol{x})_{<i} $$ |  |
| --- | --- | --- |

bpe is not an incremental patching scheme as the same prefix can be tokenized differently depending on the continuation sequence, and therefore does not satisfy the property above <sup>3</sup> <sup>3</sup> 3 Using a special delimiter token to indicate patch boundaries can turn bpe into an incremental patching scheme but increases the byte-sequence length..

## 3 BLT Architecture

B LT is composed of a large global autoregressive language model that operates on patch representations, along with two smaller local models that encode sequences of bytes into patches and decode patch representations back into bytesÂ ([Figure 2](https://arxiv.org/html/2412.09871v1#S1.F2 "Figure 2 â€£ 1 Introduction â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")).

### 3.1 Latent Global Transformer Model

The Latent Global Transformer is an autoregressive transformer model $\mathcal{G}$ with $l_{\mathcal{G}}$ layers, which maps a sequence of latent input patch representations, $p_{j}$ into a sequence of output patch representations, $o_{j}$ . Throughout the paper, we use the subscript $j$ to denote patches and $i$ to denote bytes. The global model uses a block-causal attention mask [^13], which restricts attention to be up to and including the current patch within the current document. This model consumes the bulk of the flop s during pre-training as well as inference, and thus, choosing when to invoke it allows us to control and vary the amount of compute expended for different portions of the input and output as a function of input/output complexity.

### 3.2 Local Encoder

The Local Encoder Model, denoted by $\mathcal{E}$ , is a lightweight transformer-based model with $l_{\mathcal{E}}<<l_{\mathcal{G}}$ layers, whose main role is to efficiently map a sequence of input bytes $b_{i}$ , into expressive patch representations, $p_{j}$ . A primary departure from the transformer architecture is the addition of a cross-attention layer after each transformer layer, whose function is to pool byte representations into patch representationsÂ ([Figure 5](https://arxiv.org/html/2412.09871v1#S3.F5 "Figure 5 â€£ 3.2.1 Encoder Hash n-gram Embeddings â€£ 3.2 Local Encoder â€£ 3 BLT Architecture â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). First, the input sequence of bytes, $b_{i}$ , are embedded using a $\mathbb{R}^{256\times h_{\mathcal{E}}}$ matrix, denoted as $x_{i}$ . These embeddings are then optionally augmented with additional information in the form of hash-embeddingsÂ (Â§ [3.2.1](https://arxiv.org/html/2412.09871v1#S3.SS2.SSS1 "3.2.1 Encoder Hash n-gram Embeddings â€£ 3.2 Local Encoder â€£ 3 BLT Architecture â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). A series of alternating transformer and cross-attention layersÂ (Â§ [3.2.2](https://arxiv.org/html/2412.09871v1#S3.SS2.SSS2 "3.2.2 Encoder Multi-Headed Cross-Attention â€£ 3.2 Local Encoder â€£ 3 BLT Architecture â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")) then transform these representations into patch representations, $p_{i}$ that are processed by the global transformer, $\mathcal{G}$ . The transformer layers use a local block causal attention mask; each byte attends to a fixed window of $w_{\mathcal{E}}$ preceding bytes that in general can cross the dynamic patch boundaries but can not cross document boundaries. The following subsections describe details about the embeddings and the cross-attention block.

#### 3.2.1 Encoder Hash n-gram Embeddings

A key component in creating robust, expressive representations at each step $i$ is to incorporate information about the preceding bytes. In B LT, we achieve this by modeling both the byte $b_{i}$ individually and as part of a byte n-gram. For each step $i$ , we first construct byte-grams

<table><tbody><tr><td></td><td><math><semantics><mrow><msub><mi>g</mi> <mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub> <mo>=</mo> <mrow><mo>{</mo> <msub><mi>b</mi> <mrow><mrow><mi>i</mi> <mo>âˆ’</mo> <mi>n</mi></mrow> <mo>+</mo> <mn>1</mn></mrow></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>}</mo></mrow></mrow> <annotation-xml><apply><apply><csymbol>subscript</csymbol> <ci>ğ‘”</ci> <list><ci>ğ‘–</ci> <ci>ğ‘›</ci></list></apply> <set><apply><csymbol>subscript</csymbol> <ci>ğ‘</ci> <apply><apply><ci>ğ‘–</ci> <ci>ğ‘›</ci></apply> <cn>1</cn></apply></apply> <ci>â€¦</ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘</ci> <ci>ğ‘–</ci></apply></set></apply></annotation-xml> <annotation>\displaystyle g_{i,n}=\{b_{i-n+1},\ldots,b_{i}\}</annotation> <annotation>italic_g start_POSTSUBSCRIPT italic_i, italic_n end_POSTSUBSCRIPT = { italic_b start_POSTSUBSCRIPT italic_i - italic_n + 1 end_POSTSUBSCRIPT, â€¦, italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math></td><td></td><td rowspan="1"><span>(2)</span></td></tr></tbody></table>

for each byte position $i$ and $n$ from three to eight.<sup>4</sup> <sup>4</sup> 4 We omit byte-grams of size $n$ or more when $i<n$ .

We then introduce hash $n$ -gram embeddings, that map all byte $n$ -grams via a hash function to an index in an embedding table $E_{n}^{hash}$ with a fixed size, for each size $n\in\{3,4,5,6,7,8\}$ [^3]. The resulting embedding is then added to the embedding of the byte before being normalized and passed as input to the local encoder model. We calculate the augmented embedding

<table><tbody><tr><td></td><td><math><semantics><msub><mi>e</mi> <mi>i</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ‘’</ci> <ci>ğ‘–</ci></apply></annotation-xml> <annotation>\displaystyle e_{i}</annotation> <annotation>italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mrow><mstyle><munder><mo>âˆ‘</mo> <mrow><mi>n</mi> <mo>=</mo> <mrow><mn>3</mn><mo>,</mo><mi>â€¦</mi><mo>,</mo><mn>8</mn></mrow></mrow></munder></mstyle> <mrow><msubsup><mi>E</mi> <mi>n</mi> <mrow><mi>h</mi> <mo>â¢</mo> <mi>a</mi> <mo>â¢</mo> <mi>s</mi> <mo>â¢</mo> <mi>h</mi></mrow></msubsup> <mo>â¢</mo> <mrow><mo>(</mo><mrow><mtext>Hash</mtext> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>g</mi> <mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘¥</ci> <ci>ğ‘–</ci></apply> <apply><apply><csymbol>subscript</csymbol> <apply><ci>ğ‘›</ci> <list><cn>3</cn> <ci>â€¦</ci> <cn>8</cn></list></apply></apply> <apply><apply><csymbol>superscript</csymbol> <apply><csymbol>subscript</csymbol> <ci>ğ¸</ci> <ci>ğ‘›</ci></apply> <apply><ci>â„</ci> <ci>ğ‘</ci> <ci>ğ‘ </ci> <ci>â„</ci></apply></apply> <apply><ci><mtext>Hash</mtext></ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘”</ci> <list><ci>ğ‘–</ci> <ci>ğ‘›</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=x_{i}+\sum_{n=3,...,8}E_{n}^{hash}(\text{Hash}(g_{i,n}))</annotation> <annotation>= italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + âˆ‘ start_POSTSUBSCRIPT italic_n = 3, â€¦, 8 end_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h italic_a italic_s italic_h end_POSTSUPERSCRIPT ( Hash ( italic_g start_POSTSUBSCRIPT italic_i, italic_n end_POSTSUBSCRIPT ) )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(3)</span></td></tr></tbody><tbody><tr><td></td><td><math><semantics><mrow><mtext>where, Hash</mtext> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>g</mi> <mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow> <annotation-xml><apply><ci><mtext>where, Hash</mtext></ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘”</ci> <list><ci>ğ‘–</ci> <ci>ğ‘›</ci></list></apply></apply></annotation-xml> <annotation>\displaystyle\text{where, Hash}(g_{i,n})</annotation> <annotation>where, Hash ( italic_g start_POSTSUBSCRIPT italic_i, italic_n end_POSTSUBSCRIPT )</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><mtext>RollPolyHash</mtext> <mo>â¢</mo> <mrow><mrow><mo>(</mo><msub><mi>g</mi> <mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow> <mo>%</mo></mrow> <mo>â¢</mo> <mrow><mo>|</mo> <msubsup><mi>E</mi> <mi>n</mi> <mrow><mi>h</mi> <mo>â¢</mo> <mi>a</mi> <mo>â¢</mo> <mi>s</mi> <mo>â¢</mo> <mi>h</mi></mrow></msubsup> <mo>|</mo></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><ci><mtext>RollPolyHash</mtext></ci> <apply><csymbol>percent</csymbol> <apply><csymbol>subscript</csymbol> <ci>ğ‘”</ci> <list><ci>ğ‘–</ci> <ci>ğ‘›</ci></list></apply></apply> <apply><apply><csymbol>superscript</csymbol> <apply><csymbol>subscript</csymbol> <ci>ğ¸</ci> <ci>ğ‘›</ci></apply> <apply><ci>â„</ci> <ci>ğ‘</ci> <ci>ğ‘ </ci> <ci>â„</ci></apply></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=\text{RollPolyHash}(g_{i,n})\%|E_{n}^{hash}|</annotation> <annotation>= RollPolyHash ( italic_g start_POSTSUBSCRIPT italic_i, italic_n end_POSTSUBSCRIPT ) % | italic_E start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h italic_a italic_s italic_h end_POSTSUPERSCRIPT |</annotation></semantics></math></td><td></td><td rowspan="1"><span>(4)</span></td></tr></tbody></table>

We normalize $e_{i}$ by the number of $n$ -grams sizes plus one and use RollPolyHash as defined in AppendixÂ  [13](https://arxiv.org/html/2412.09871v1#S13 "13 Rolling Polynomial Hashing â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). In SectionÂ  [7](https://arxiv.org/html/2412.09871v1#S7 "7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we ablate the effects of $n$ -gram hash embeddings with different values for $n$ and embedding table size on flop -controlled scaling law trends. In addition to hash $n$ -gram embeddings, we also experimented with frequency based $n$ -gram embeddings, and we provide details of this exploration in AppendixÂ  [14](https://arxiv.org/html/2412.09871v1#S14 "14 Frequency-based n-gram Embedddings â€£ Byte Latent Transformer: Patches Scale Better Than Tokens").

![Refer to caption](https://arxiv.org/html/x5.png)

Figure 5: The local encoder uses a cross-attention block with patch representations as queries, and byte representations as keys/values to encode byte representations into patch representations. The local decoder uses a similar block but with the roles reversed i.e. byte representations are now the queries and patch representations are the keys/values. Here we use Cross-Attn k = 2 ğ‘˜ k=2 italic\_k = 2.

#### 3.2.2 Encoder Multi-Headed Cross-Attention

We closely follow the input cross-attention module of the Perceiver architecture [^22], with the main difference being that latent representations correspond to variable patch representations as opposed to a fixed set of latent representations ([Figure 5](https://arxiv.org/html/2412.09871v1#S3.F5 "Figure 5 â€£ 3.2.1 Encoder Hash n-gram Embeddings â€£ 3.2 Local Encoder â€£ 3 BLT Architecture â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), and only attend to the bytes that make up the respective patch. The module comprises a query vector, corresponding to each patch $p_{j}$ , which is initialized by pooling the byte representations corresponding to patch $p_{j}$ , followed by a linear projection, $\mathcal{E}_{C}\in\mathbb{R}^{h_{\mathcal{E}}\times(h_{\mathcal{E}}\times U_{%
\mathcal{E}})}$ , where $U_{\mathcal{E}}$ is the number of encoder cross-attention heads. Formally, if we let $f_{\text{bytes}}(p_{j})$ denote the sequence of bytes corresponding to patch, $p_{j}$ , then we calculate

<table><tbody><tr><td></td><td><math><semantics><msub><mi>P</mi> <mrow><mn>0</mn><mo>,</mo><mi>j</mi></mrow></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ‘ƒ</ci> <list><cn>0</cn> <ci>ğ‘—</ci></list></apply></annotation-xml> <annotation>\displaystyle P_{0,j}</annotation> <annotation>italic_P start_POSTSUBSCRIPT 0, italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <msub><mi>â„°</mi> <mi>C</mi></msub> <mrow><mo>(</mo><msub><mi>f</mi> <mtext>bytes</mtext></msub> <mrow><mo>(</mo><mrow><mo>(</mo><msub><mi>p</mi> <mi>j</mi></msub><mo>)</mo></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi> <mtext>is a pooling function</mtext></mrow></mrow> <annotation>\displaystyle=\mathcal{E}_{C}(f_{\text{bytes}}((p_{j})),f\leavevmode\nobreak\ % \text{is a pooling function}</annotation> <annotation>= caligraphic_E start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT bytes end_POSTSUBSCRIPT ( ( italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ), italic_f is a pooling function</annotation></semantics></math></td><td></td><td rowspan="1"><span>(5)</span></td></tr></tbody><tbody><tr><td></td><td><math><semantics><msub><mi>P</mi> <mi>l</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ‘ƒ</ci> <ci>ğ‘™</ci></apply></annotation-xml> <annotation>\displaystyle P_{l}</annotation> <annotation>italic_P start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><msub><mi>P</mi> <mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow></msub> <mo>+</mo> <mrow><msub><mi>W</mi> <mi>o</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><mrow><mtext>softmax</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mstyle><mfrac><mrow><mi>Q</mi> <mo>â¢</mo> <msup><mi>K</mi> <mi>T</mi></msup></mrow> <msqrt><msub><mi>d</mi> <mi>k</mi></msub></msqrt></mfrac></mstyle><mo>)</mo></mrow> <mo>â¢</mo> <mi>V</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘ƒ</ci> <apply><ci>ğ‘™</ci> <cn>1</cn></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘œ</ci></apply> <apply><ci><mtext>softmax</mtext></ci> <apply><apply><ci>ğ‘„</ci> <apply><csymbol>superscript</csymbol> <ci>ğ¾</ci> <ci>ğ‘‡</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘‘</ci> <ci>ğ‘˜</ci></apply></apply></apply> <ci>ğ‘‰</ci></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=P_{l-1}+W_{o}\left(\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}% }}\right)V\right)</annotation> <annotation>= italic_P start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(6)</span></td></tr></tbody><tbody><tr><td></td><td><math><semantics><mrow><mtext>where </mtext><mo>â¢</mo> <msub><mi>Q</mi> <mi>j</mi></msub></mrow> <annotation-xml><apply><ci><mtext>where </mtext></ci><apply><csymbol>subscript</csymbol> <ci>ğ‘„</ci> <ci>ğ‘—</ci></apply></apply></annotation-xml> <annotation>\displaystyle\text{where }Q_{j}</annotation> <annotation>where italic_Q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mrow><mo>=</mo> <mrow><msub><mi>W</mi> <mi>q</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>P</mi> <mrow><mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>K</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><msub><mi>W</mi> <mi>k</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>h</mi> <mrow><mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><msub><mi>V</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><msub><mi>W</mi> <mi>v</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>h</mi> <mrow><mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow></mrow> <annotation-xml><apply><csymbol>formulae-sequence</csymbol> <apply><csymbol>absent</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘ƒ</ci> <list><apply><ci>ğ‘™</ci> <cn>1</cn></apply> <ci>ğ‘—</ci></list></apply></apply></apply> <apply><csymbol>formulae-sequence</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ¾</ci> <ci>ğ‘–</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘˜</ci></apply> <apply><csymbol>subscript</csymbol> <ci>â„</ci> <list><apply><ci>ğ‘™</ci> <cn>1</cn></apply> <ci>ğ‘–</ci></list></apply></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘‰</ci> <ci>ğ‘–</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘£</ci></apply> <apply><csymbol>subscript</csymbol> <ci>â„</ci> <list><apply><ci>ğ‘™</ci> <cn>1</cn></apply> <ci>ğ‘–</ci></list></apply></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=W_{q}(P_{l-1,j}),K_{i}=W_{k}(h_{l-1,i}),V_{i}=W_{v}(h_{l-1,i})</annotation> <annotation>= italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_P start_POSTSUBSCRIPT italic_l - 1, italic_j end_POSTSUBSCRIPT ), italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_l - 1, italic_i end_POSTSUBSCRIPT ), italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_l - 1, italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(7)</span></td></tr></tbody><tbody><tr><td></td><td><math><semantics><msub><mi>h</mi> <mi>l</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>â„</ci> <ci>ğ‘™</ci></apply></annotation-xml> <annotation>\displaystyle h_{l}</annotation> <annotation>italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><msub><mtext>Encoder-Transformer-Layer</mtext> <mi>l</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>h</mi> <mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci><mtext>Encoder-Transformer-Layer</mtext></ci> <ci>ğ‘™</ci></apply> <apply><csymbol>subscript</csymbol> <ci>â„</ci> <apply><ci>ğ‘™</ci> <cn>1</cn></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=\text{Encoder-Transformer-Layer}_{l}(h_{l-1})</annotation> <annotation>= Encoder-Transformer-Layer start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(8)</span></td></tr></tbody></table>

where $P\in\mathbb{R}^{n_{p}\times h_{\mathcal{G}}}$ represents $n_{p}$ patch representations to be processed by the global model, which is initialized by pooling together the byte embeddings $e_{i}$ corresponding to each patch $p_{j}$ . $W_{q}$ , $W_{k}$ , $W_{v}$ and $W_{o}$ are the projections corresponding to the queries, keys, values, and output where the keys and values are projections of byte representations $h_{i}$ from the previous layer ( $e_{i}$ for the first layer). We use a masking strategy specific to patching where each query $Q_{j}$ only attends to the keys and values that correspond to the bytes in patch $j$ . Because we use multi-headed attention over $Q,K$ and $V$ and patch representations are typically of larger dimension ( $h_{\mathcal{G}}$ ) than $h_{\mathcal{E}}$ , we maintain $P_{l}$ as multiple heads of dimension $h_{\mathcal{E}}$ when doing cross-attention, and later, concat these representations into $h_{\mathcal{G}}$ dimensions. Additionally, we use a pre-LayerNorm on the queries, keys and values and no positional embeddings are used in this cross-attention module. Finally, we use a residual connection around the cross-attention block.

### 3.3 Local Decoder

Similar to the local encoder, the local decoder $\mathcal{D}$ is a lightweight transformer-based model with $l_{\mathcal{D}}<<l_{\mathcal{G}}$ layers, that decodes a sequence of global patch representations $o_{j}$ , into raw bytes, $y_{i}$ . The local decoder predicts a sequence of raw bytes, as a function of previously decoded bytes, and thus, takes as input the hidden representations produced by the local encoder for the byte-sequence. It applies a series of $l_{\mathcal{D}}$ alternating layers of cross attention and transformer layers. The cross-attention layer in the decoder is applied before the transformer layer to first create byte representations from the patch representations, and the local decoder transformer layer operates on the resulting byte sequence.

#### 3.3.1 Decoder Multi-headed Cross-Attention

In the decoder cross-attention, the roles of the queries and key/values are interchanged i.e. the byte-representations are now the queries, and the patch representations are now the key/values. The initial byte-representations for the cross-attention are initialized as the byte embeddings from the last encoder layer i.e. $h_{l_{\mathcal{E}}}$ . The subsequent byte-representations for layer $l$ , $d_{l,i}$ are computed as:

<table><tbody><tr><td></td><td><math><semantics><msub><mi>D</mi> <mn>0</mn></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ·</ci> <cn>0</cn></apply></annotation-xml> <annotation>\displaystyle D_{0}</annotation> <annotation>italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <msub><mi>h</mi> <msub><mi>l</mi> <mi>â„°</mi></msub></msub></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><csymbol>subscript</csymbol> <ci>â„</ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘™</ci> <ci>â„°</ci></apply></apply></apply></annotation-xml> <annotation>\displaystyle=h_{l_{\mathcal{E}}}</annotation> <annotation>= italic_h start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math></td><td></td><td rowspan="1"><span>(9)</span></td></tr></tbody><tbody><tr><td></td><td><math><semantics><msub><mi>B</mi> <mi>l</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğµ</ci> <ci>ğ‘™</ci></apply></annotation-xml> <annotation>\displaystyle B_{l}</annotation> <annotation>italic_B start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mrow><mo>=</mo> <mrow><msub><mi>D</mi> <mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow></msub> <mo>+</mo> <mrow><msub><mi>W</mi> <mi>o</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><mrow><mtext>softmax</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mstyle><mfrac><mrow><mi>Q</mi> <mo>â¢</mo> <msup><mi>K</mi> <mi>T</mi></msup></mrow> <msqrt><msub><mi>d</mi> <mi>k</mi></msub></msqrt></mfrac></mstyle><mo>)</mo></mrow> <mo>â¢</mo> <mi>V</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ·</ci> <apply><ci>ğ‘™</ci> <cn>1</cn></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘œ</ci></apply> <apply><ci><mtext>softmax</mtext></ci> <apply><apply><ci>ğ‘„</ci> <apply><csymbol>superscript</csymbol> <ci>ğ¾</ci> <ci>ğ‘‡</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘‘</ci> <ci>ğ‘˜</ci></apply></apply></apply> <ci>ğ‘‰</ci></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=D_{l-1}+W_{o}\left(\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}% }}\right)V\right),</annotation><annotation>= italic_D start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V ),</annotation></semantics></math></td><td></td><td rowspan="1"><span>(10)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mrow><mrow><mtext>where </mtext><mo>â¢</mo> <msub><mi>Q</mi> <mi>i</mi></msub></mrow> <mo>=</mo> <mrow><msub><mi>W</mi> <mi>q</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>d</mi> <mrow><mrow><mi>l</mi> <mo>âˆ’</mo> <mn>1</mn></mrow><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>K</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><msub><mi>W</mi> <mi>k</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><mrow><msub><mi>ğ’Ÿ</mi> <mi>C</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>o</mi> <mi>j</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><msub><mi>V</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><msub><mi>W</mi> <mi>v</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><mrow><msub><mi>ğ’Ÿ</mi> <mi>C</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>o</mi> <mi>j</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow> <annotation-xml><apply><csymbol>formulae-sequence</csymbol> <apply><apply><ci><mtext>where </mtext></ci><apply><csymbol>subscript</csymbol> <ci>ğ‘„</ci> <ci>ğ‘–</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘‘</ci> <list><apply><ci>ğ‘™</ci> <cn>1</cn></apply> <ci>ğ‘–</ci></list></apply></apply></apply> <apply><csymbol>formulae-sequence</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ¾</ci> <ci>ğ‘–</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘˜</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ’Ÿ</ci> <ci>ğ¶</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘œ</ci> <ci>ğ‘—</ci></apply></apply></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘‰</ci> <ci>ğ‘–</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘Š</ci> <ci>ğ‘£</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ’Ÿ</ci> <ci>ğ¶</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘œ</ci> <ci>ğ‘—</ci></apply></apply></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle\text{where }Q_{i}=W_{q}(d_{l-1,i}),K_{i}=W_{k}(\mathcal{D}_{C}(o% _{j})),V_{i}=W_{v}(\mathcal{D}_{C}(o_{j}))</annotation> <annotation>where italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d start_POSTSUBSCRIPT italic_l - 1, italic_i end_POSTSUBSCRIPT ), italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ), italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(11)</span></td></tr></tbody><tbody><tr><td></td><td><math><semantics><msub><mi>D</mi> <mi>l</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ·</ci> <ci>ğ‘™</ci></apply></annotation-xml> <annotation>\displaystyle D_{l}</annotation> <annotation>italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><msub><mtext>Decoder-Transformer-layer</mtext> <mi>l</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>B</mi> <mi>l</mi></msub><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci><mtext>Decoder-Transformer-layer</mtext></ci> <ci>ğ‘™</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğµ</ci> <ci>ğ‘™</ci></apply></apply></apply></annotation-xml> <annotation>\displaystyle=\text{Decoder-Transformer-layer}_{l}(B_{l})</annotation> <annotation>= Decoder-Transformer-layer start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_B start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(12)</span></td></tr></tbody></table>

where once again, $W_{k},W_{v}$ are key/value projection matrices that operate on a linear transformation and split operation $\mathcal{D}_{C}$ , applied to the final patch representations $o_{j}$ from the global model, $W_{q}$ is a query projection matrices operating on byte representations $d_{l-1}$ from the previous decoder transformer layer (or $h_{l_{\mathcal{E}}}$ for the first layer), and $W_{o}$ is the output projection matrix, thus making $B\in\mathbb{R}^{h_{\mathcal{D}}\times n_{b}}$ , where $n_{b}$ is the number of output bytes. The next decoder representations $D_{l}$ are computed using a decoder transformer layer on the output of the cross-attention block, $B$ . As in the local encoder cross-attention, we use multiple heads in the attention, use pre LayerNorms, no positional embeddings, and a residual connection around the cross-attention module.

## 4 Experimental Setup

We carefully design controlled experiments to compare B LT with tokenization based models with particular attention to not give B LT any advantages from possibly using longer sequence contexts.

### 4.1 Pre-training Datasets

All model scales that we experiment in this paper are pre-trained on two datasets: 1) The Llama 2 dataset [^43], which comprises 2 trillion tokens collected from a variety of publicly available sources, which are subsequently cleaned and filtered to improve quality; and 2) B LT-1T: A new dataset with 1 trillion tokens gathered from various public sources, and also including a subset of the pre-training data released by Datacomp-LM [^28]. The former is used for scaling law experiments on optimal number of tokens as determined by [^13] to determine the best architectural choices for B LT, while the latter is used for a complete pre-training run to compare with Llama 3 on downstream tasks. Neither of these datasets include any data gathered from Meta products or services. Furthermore, for baseline experiments for tokenizer-based models, we use the Llama 3 tokenizer with a vocabulary size of 128K tokens, which produced stronger baseline performance that the Llama 2 tokenizer in our experiments.

### 4.2 Entropy Model

The entropy model in our experiments is a byte level language model trained on the same training distribution as the full B LT model. Unless otherwise mentioned, we use a transformer with 100M parameters, 14 layers, and a hidden dimensionality of 512, and sliding window attention of 512 bytes. The remaining hyperparameters are the same as in our local and global transformers. We experimented with different model sizes, receptive fields, and architectures as discussed in [section 7](https://arxiv.org/html/2412.09871v1#S7 "7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). In particular, when the receptive field of the model is small enough, the trained entropy model can be encoded in an efficient lookup table.

### 4.3 Entropy Threshold and Equalizing Context Length

For models using entropy-based patching, we estimate a patching threshold that achieves a desired average patch size on the pretraining data mix. In B LT, unlike with tokenization, the patch size can be arbitrarily chosen having significant implications on the context size used by the model. To maintain the same average context length and avoid giving larger patch sizes unfair advantage, we ensure that the number of bytes in each batch remains constant in expectation. This means that we reduce the sequence length of models with larger patch sizes. On Llama 2 data, we use a 8k byte context while on the B LT-1T dataset we increase the context to 16k bytes on average while maintaining the same batch size of 16M bytes on average.

While the average batch size is constant, when loading batches of data, dynamic patching methods yield different ratios of bytes to patches. For efficiency reasons, our implementation of B LT training packs batches of patches to avoid padding steps in the more expensive latent transformer. This ensures that every batch has the same number of patches. During training we pad and possibly truncate byte sequences to 12k and 24k bytes respectively for Llama 2 and B LT-1T datasets, to avoid memory spikes from sequences with unusually large patches.

### 4.4 Entropy Model Context

Empirically, we find that using entropy patching yields progressively larger patches in structured content like multiple choice tasks (see patching on an MMLU example in [Figure 9](https://arxiv.org/html/2412.09871v1#S15.F9 "Figure 9 â€£ 15 Entropy Patching Example from MMLU â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")) which are often very repetitive. These variations are caused by lower entropy on the repeated content found in the entropy model context. So for the large scale run of B LT-Entropy with patch size 4.5, we reset the entropy context with new lines and use approximate monontonicity constraint as it suffers less from "entropy drift" from changes in context length. This change only affects how we compute entropies, but we still follow the same procedure to identify the value of the entropy threshold.

### 4.5 FLOPs Estimation

We largely follow the equations for computation of transformer flop s from Chinchilla [^21] comprising flop s for the feed-forward layers, qkvo projections in the self-attention layer, and computation of attention and output projection. A notable difference is that we assume the input embedding layer is implemented as an efficient lookup instead of a dense matrix multiplication, therefore becoming a 0- flop operation. Following previous work, we estimate that the backwards pass has twice the number of flop s as the forward pass.

To compute flop s per byte for B LT models, we add up the flop s for the local encoder transformer, the global latent transformer, and the local decoder transformer, together with the cross attention blocks in the encoder and the decoder:

<table><tbody><tr><td></td><td><math><semantics><msub><mtext>FL</mtext> <mrow><mtext>B</mtext> <mtext>LT</mtext></mrow></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci><mtext>FL</mtext></ci> <ci><mrow><mtext>B</mtext> <mtext>LT</mtext></mrow></ci></apply></annotation-xml> <annotation>\displaystyle\text{FL}_{\text{{{B}LT}{}}}</annotation> <annotation>FL start_POSTSUBSCRIPT smallcaps_B LT end_POSTSUBSCRIPT</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mtext>Transf. FL</mtext> <mrow><mo>(</mo><msub><mi>h</mi> <mi>ğ’¢</mi></msub><mo>,</mo><msub><mi>l</mi> <mi>ğ’¢</mi></msub><mo>,</mo><mi>m</mi> <mo>=</mo> <msub><mi>n</mi> <mrow><mi>c</mi> <mo>â¢</mo> <mi>t</mi> <mo>â¢</mo> <mi>x</mi></mrow></msub> <mo>/</mo> <msub><mi>n</mi> <mi>p</mi></msub><mo>,</mo><mi>V</mi> <mo>=</mo> <mn>0</mn><mo>)</mo></mrow> <mo>/</mo> <msub><mi>n</mi> <mi>p</mi></msub></mrow> <annotation>\displaystyle=\text{Transf. FL}(h_{\mathcal{G}},l_{\mathcal{G}},m=n_{ctx}/n_{p% },V=0)/n_{p}</annotation> <annotation>= Transf. FL ( italic_h start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT, italic_l start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT, italic_m = italic_n start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT / italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, italic_V = 0 ) / italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math></td><td></td><td rowspan="1"><span>(13)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mtext>Transf. FL</mtext> <mrow><mo>(</mo><msub><mi>h</mi> <mi>â„°</mi></msub><mo>,</mo><msub><mi>l</mi> <mi>â„°</mi></msub><mo>,</mo><mi>m</mi> <mo>=</mo> <msub><mi>w</mi> <mi>â„°</mi></msub><mo>,</mo><mi>V</mi> <mo>=</mo> <mn>0</mn><mo>)</mo></mrow></mrow> <annotation>\displaystyle+\text{Transf. FL}(h_{\mathcal{E}},l_{\mathcal{E}},m=w_{\mathcal{% E}},V=0)</annotation> <annotation>+ Transf. FL ( italic_h start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT, italic_l start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT, italic_m = italic_w start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT, italic_V = 0 )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(14)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mtext>Transf. FL</mtext> <mrow><mo>(</mo><msub><mi>h</mi> <mi>ğ’Ÿ</mi></msub><mo>,</mo><msub><mi>l</mi> <mi>ğ’Ÿ</mi></msub><mo>,</mo><mi>m</mi> <mo>=</mo> <msub><mi>w</mi> <mi>ğ’Ÿ</mi></msub><mo>,</mo><mi>V</mi> <mo>=</mo> <mn>256</mn><mo>)</mo></mrow></mrow> <annotation>\displaystyle+\text{Transf. FL}(h_{\mathcal{D}},l_{\mathcal{D}},m=w_{\mathcal{% D}},V=256)</annotation> <annotation>+ Transf. FL ( italic_h start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT, italic_l start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT, italic_m = italic_w start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT, italic_V = 256 )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(15)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mtext>Cross Attn. FL</mtext> <mrow><mo>(</mo><msub><mi>h</mi> <mi>â„°</mi></msub><mo>,</mo><msub><mi>l</mi> <mi>â„°</mi></msub><mo>,</mo><mi>m</mi> <mo>=</mo> <msub><mi>n</mi> <mi>p</mi></msub><mo>,</mo><mi>r</mi> <mo>=</mo> <msub><mi>n</mi> <mi>p</mi></msub> <mo>/</mo> <mi>k</mi><mo>)</mo></mrow> <mo>Ã—</mo> <mi>k</mi> <mo>/</mo> <msub><mi>n</mi> <mi>p</mi></msub></mrow> <annotation>\displaystyle+\text{Cross Attn. FL}(h_{\mathcal{E}},l_{\mathcal{E}},m=n_{p},r=% n_{p}/k)\times k/n_{p}</annotation> <annotation>+ Cross Attn. FL ( italic_h start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT, italic_l start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT, italic_m = italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, italic_r = italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT / italic_k ) Ã— italic_k / italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math></td><td></td><td rowspan="1"><span>(16)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mtext>Cross Attn. FL</mtext> <mrow><mo>(</mo><msub><mi>h</mi> <mi>ğ’Ÿ</mi></msub><mo>,</mo><msub><mi>l</mi> <mi>ğ’Ÿ</mi></msub><mo>,</mo><mi>m</mi> <mo>=</mo> <mi>k</mi><mo>,</mo><mi>r</mi> <mo>=</mo> <mi>k</mi> <mo>/</mo> <msub><mi>n</mi> <mi>p</mi></msub><mo>)</mo></mrow></mrow> <annotation>\displaystyle+\text{Cross Attn. FL}(h_{\mathcal{D}},l_{\mathcal{D}},m=k,r=k/n_% {p})</annotation> <annotation>+ Cross Attn. FL ( italic_h start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT, italic_l start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT, italic_m = italic_k, italic_r = italic_k / italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(17)</span></td></tr></tbody></table>

where $n_{ctx}$ is the sequence length in bytes, $n_{p}$ is the patch size, $r$ is the ratio of queries to key/values, $k$ is the ratio of patch-dimension to byte-dimension i.e. the number of local model splits that concatenate to form a global model representation ( $k=2$ in [Figure 5](https://arxiv.org/html/2412.09871v1#S3.F5 "Figure 5 â€£ 3.2.1 Encoder Hash n-gram Embeddings â€£ 3.2 Local Encoder â€£ 3 BLT Architecture â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). $V$ corresponds to the vocabulary size for the output projection, which is only used in the local decoder. Depending on whether a module is applied on the byte or patch sequence, the attention uses a different context length, $m$ . We modify the attention flop s accordingly for each component. The exact equations for flop s computation for Transformer-FLOPs and Cross-Attention FLOPs are provided in AppendixÂ  [12](https://arxiv.org/html/2412.09871v1#S12 "12 FLOPs Equations â€£ Byte Latent Transformer: Patches Scale Better Than Tokens").

### 4.6 Bits-Per-Byte Estimation

Perplexity only makes sense in the context of a fixed tokenizer as it is a measure of the uncertainty for each token. When comparing byte and token-level models, following previous work [^47], we instead report Bits-Per-Byte (BPB), a tokenizer independent version of perplexity. Specifically:

<table><tbody><tr><td></td><td><math><semantics><mrow><mtext>BPB</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <annotation-xml><apply><ci><mtext>BPB</mtext></ci> <ci>ğ‘¥</ci></apply></annotation-xml> <annotation>\displaystyle\text{BPB}(x)</annotation> <annotation>BPB ( italic_x )</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mstyle><mfrac><mrow><msub><mi>â„’</mi> <mrow><mi>C</mi> <mo>â¢</mo> <mi>E</mi></mrow></msub> <mo>â¢</mo> <mrow><mo>(</mo><mi>ğ’™</mi><mo>)</mo></mrow></mrow> <mrow><mrow><mi>ln</mi> <mo>â¡</mo> <mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></mrow> <mo>â‹…</mo> <msub><mi>n</mi> <mtext>bytes</mtext></msub></mrow></mfrac></mstyle></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><apply><csymbol>subscript</csymbol> <ci>â„’</ci> <apply><ci>ğ¶</ci> <ci>ğ¸</ci></apply></apply> <ci>ğ’™</ci></apply> <apply><ci>â‹…</ci> <apply><cn>2</cn></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘›</ci> <ci><mtext>bytes</mtext></ci></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=\frac{\mathcal{L}_{CE}(\boldsymbol{x})}{\ln(2)\cdot n_{\text{% bytes}}}</annotation> <annotation>= divide start_ARG caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT ( bold_italic_x ) end_ARG start_ARG roman_ln ( 2 ) â‹… italic_n start_POSTSUBSCRIPT bytes end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td><td></td><td rowspan="1"><span>(18)</span></td></tr></tbody></table>

where the uncertainty over the data $\boldsymbol{x}$ as measured by the sum of the cross-entropy loss is normalized by the total number of bytes in $\boldsymbol{x}$ and a constant.

### 4.7 Transformer Architecture Hyperparameters

For all the transformer blocks in B LT, i.e. both local and global models, we largely follow the architecture of Llama 3 [^13]; we use the SwiGLU activation function [^38] in the feed-forward layers, rotary positional embeddings (RoPE) [^40] with $\theta=500000$ [^46] only in self-attention layers, and RMSNorm [^50] for layer normalization. We use Flash attention [^12] for all self-attention layers that use fixed-standard attention masks such as block causal or fixed-window block causal, and a window size of 512 for fixed-width attention masks. Since our cross-attention layers involve dynamic patch-dependent masks, we use Flex Attention <sup>5</sup> <sup>5</sup> 5 [https://pytorch.org/blog/flexattention](https://pytorch.org/blog/flexattention) to produce fused implementations and significantly speed up training.

### 4.8 BLT-Specific Hyperparameters

To study the effectiveness of B LT models, we conduct experiments along two directions, scaling trends, and downstream task evaluations, and we consider models at different scales: 400M, 1B, 2B, 4B and 8B for these experiments. The architecture hyperparameters for these models are presented in AppendixÂ TableÂ  [10](https://arxiv.org/html/2412.09871v1#S11.T10 "Table 10 â€£ 11 Model Hyper Parameters â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). We use max-pooling to initialize the queries for the first cross-attention layer in the local encoder. We use $500,000$ hashes with a single hash function, with n-gram sizes ranging from 3 to 8, for all B LT models. We use a learning rate of $4e-4$ for all models. The choice of matching learning rate between token and B LT models follows a hyperparameter search between $1e-3$ and $1e-4$ at 400M and 1B model scales showing the same learning rate is optimal. For scaling trends on Llama-2 data, we use training batch-sizes as recommended byÂ  [^13] or its equivalent in bytes. For optimization, we use the AdamW optimizer [^31] with $\beta_{1}$ set to 0.9 and $\beta_{2}$ to 0.95, with an $\epsilon=10^{-8}$ . We use a linear warm-up of 2000 steps with an cosine decay schedule of the learning rate to 0, we apply a weight decay of 0.1, and global gradient clipping at a threshold of 1.0.

## 5 Scaling Trends

We present a holistic picture of the scaling trends of byte-level models that can inform further scaling of B LT models. Our scaling study aims to address the limitations of previous research on byte-level models in the following ways: (a) We compare trends for the compute-optimal training regime, (b) We train matching 8B models on non-trivial amounts of training data (up to 1T tokens/4T bytes) and evaluate on downstream tasks, and (c) We measure scaling trends in inference-cost controlled settings. In a later section, we will investigate specific advantages from modeling byte-sequences.

![Refer to caption](https://arxiv.org/html/x6.png)

Figure 6: 13

### 5.1 Parameter Matched Compute Optimal Scaling Trends

Using the Llama 2 dataset, we train various compute-optimal bpe and B LT models across four different sizes, ranging from 1B to 8B parameters. We then plot the training flop s against language modeling performance on a representative subset of the training data mixture. The bpe models are trained using the optimal ratio of model parameters to training data, as determined by Llama 3 [^13]. This compute-optimal setup is theoretically designed to achieve the best performance on the training dataset within a given training budget [^21], providing a robust baseline for our model. For each bpe model, we also train a corresponding B LT model on the same data, using a Latent Transformer that matches the size and architecture of the corresponding bpe Transformer.

As illustrated in [Figure 6](https://arxiv.org/html/2412.09871v1#S5.F6 "Figure 6 â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") (right), B LT models either match or outperform their bpe counterparts and this trend holds as we scale model size and flop s. To the best of our knowledge, B LT is the first byte-level Transformer architecture to achieve matching scaling trends with BPE-based models at compute optimal regimes. This therefore validates our assumption that the optimal ratio of parameters to training compute for bpe also applies to B LT, or at least it is not too far off.

Both architectural improvements and dynamic patching are crucial to match bpe scaling trends. In Â  [Figure 6](https://arxiv.org/html/2412.09871v1#S5.F6 "Figure 6 â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") (left), we compare space-patching-based models against Llama 3. We approximate SpaceByte [^39] using B LT space-patching without n-gram embeddings and cross-attention. Although SpaceByte improves over Megabyte, it remains far from Llama 3. In [Figure 6](https://arxiv.org/html/2412.09871v1#S5.F6 "Figure 6 â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") (right), we illustrate the improvements from both architectural changes and dynamic patching. B LT models perform on par with state-of-the-art tokenizer-based models such as Llama 3, at scale.

We also observe the effects of the choice of tokenizer on performance for tokenizer-based models, i.e., models trained with the Llama-3 tokenizer outperform those trained using the Llama-2 tokenizer on the same training data.

Finally, our B LT architecture trends between Llama 2 and 3 when using significantly larger patch sizes. The bpe tokenizers of Llama 2 and 3 have an average token size of 3.7 and 4.4 bytes. In contrast, B LT can achieve similar scaling trends with an average patch size of 6 and even 8 bytes. Inference flop are inversely proportional to the average patch size, so using a patch size of 8 bytes would lead to nearly 50% inference flop savings. Models with larger patch sizes also seem to perform better as we scale model and data size. B LT with patch size of 8 starts at a significantly worse point compared to bpe Llama 2 at 1B but ends up better than bpe at 7B scale. This suggests that such patch sizes might perform better at even larger scales and possibly that even larger ones could be feasible as model size and training compute grow.

### 5.2 Beyond Compute Optimal Task Evaluations

To assess scaling properties further, we train an 8B B LT model beyond the compute optimal ratio on the B LT-1T dataset, a larger higher-quality dataset, and measure performance on a suite of standard classification and generation benchmarks. For evaluation, we select the following common sense reasoning, world knowledge, and code generation tasks:

##### Classification tasks

include ARC-Easy (0-shot) [^10], Arc-Challenge (0-shot) [^10], HellaSwag (0-shot) [^49], PIQA (0-shot) [^4], and MMLU (5-shot) [^20]. We employ a prompt-scoring method, calculating the likelihood over choice characters, and report the average accuracy.

##### Coding related generation tasks:

We report pass@1 scores on MBPP (3-shot) [^2] and HumanEval (0-shot) [^6], to evaluate the ability of LLMs to generate Python code.

|  | \| Llama 3 \| \| --- \| \| 1T Tokens \| | \| B LT-Space \| \| --- \| \| 6T Bytes \| | \| B LT-Entropy \| \| --- \| \| 4.5T Bytes \| |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Arc-E | 77.6 | 75.4 | 79.6 |
| Arc-C | 53.3 | 49.8 | 52.1 |
| HellaSwag | 79.1 | 79.6 | 80.6 |
| PIQA | 80.7 | 81.1 | 80.6 |
| MMLU | 58.1 | 54.8 | 57.4 |
| MBPP | 40.2 | 37.6 | 41.8 |
| HumanEval | 31.1 | 27.4 | 35.4 |
| Average | 60.0 | 58.0 | 61.1 |
| Bytes/Patch on Train Mix | 4.4 | 6.1 | 4.5 |

Table 1: Comparison of flop -matched B LT 8B models trained on the B LT-1T dataset comprising high-quality tokens of text and code from publicly available sources, with baseline models using the Llama 3 tokenizer. B LT performs better than Llama 3 on average, and depending on the patching scheme, achieves significant flop s savings with a minor reduction in performance.

In [Table 1](https://arxiv.org/html/2412.09871v1#S5.T1 "Table 1 â€£ Coding related generation tasks: â€£ 5.2 Beyond Compute Optimal Task Evaluations â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we compare three models trained on the B LT-1T dataset: a bpe Llama 3 tokenizer-based model,<sup>6</sup> <sup>6</sup> 6 We choose the Llama 3 tokenizer with its 128k vocabulary as it performs better than Llama 2â€™s 32k vocabulary. and two variants of the B LT model. One employing a space-patching scheme (B LT-Space) and another utilizing an entropy-based patching scheme (B LT-Entropy). with approx. monotonicity constraint and reset the context of the entropy model with new lines (as discussed inÂ  [subsection 4.4](https://arxiv.org/html/2412.09871v1#S4.SS4 "4.4 Entropy Model Context â€£ 4 Experimental Setup â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")). All three models are trained with an equivalent flop budget. However, with B LT-Entropy we additionally make an inference time adjustment of the entropy threshold from 0.6 to 0.1 which we find to improve task performance at the cost of more inference steps.

The B LT-Entropy model outperforms the Llama 3 model on 4 out of 7 tasks while being trained on the same number of bytes. This improvement is like due to a combination of (1) a better use of training compute via dynamic patching, and (2) the direct modeling of byte-level information as opposed to tokens.

On the other hand, B LT-Space underperforms the Llama 3 tokenizer on all but one task, but it achieves a significant reduction in inference flop s with its larger average patch size of 6 bytes. In comparison, the bpe and entropy-patching based models have roughly equivalent average patch size of approximately 4.5 bytes on the training data mix. With the same training budget, the larger patch size model covers 30% more data than the other two models which might push B LT further away from the compute-optimal point.

| Llama 2 | Llama 3 | Entropy ps=6 | Entropy ps=8 | Inference flop s | Compute Optimal (Bytes) | Crossover (Bytes) |
| --- | --- | --- | --- | --- | --- | --- |
| 470m | 450m | 610m (1.2x) | 760m (1.6x) | 3.1E8 | 50B | 150B |
| 3.6B | 3.9B | 5.2B (1.3x) | 6.6B (1.7x) | 2.1E9 | 400B | 1T |

Table 2: Details of models used in the fixed-inference scaling study. We report non-embedding parameters for each model and their relative number compared to Llama 2. We pick model sizes with equal inference flop s per byte. We also indicate BPEâ€™s compute-optimal training data quantity and the crossover point where B LT surpasses BPE as seen in [Figure 1](https://arxiv.org/html/2412.09871v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") (both expressed in bytes of training data). This point is achieved at much smaller scales compared to many modern training budgets.

### 5.3 Patches Scale Better Than Tokens

With B LT models, we can simultaneously increase model size and patch size while maintaining the same training and inference flop budget and keeping the amount of training data constant. Arbitrarily increasing the patch size is a unique feature of patch-based models which break free of the efficiency tradeoffs of fixed-vocabulary token-based models, as discussed in SectionÂ  [2.4](https://arxiv.org/html/2412.09871v1#S2.SS4 "2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching â€£ 2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). Longer patch sizes save compute, which can be reallocated to grow the size of the global latent transformer, because it is run less often.

|  | \| Llama 3 \| \| --- \| \| (1T tokens) \| | \| Llama 3.1 \| \| --- \| \| (16T tokens) \| | \| B LT \| \| --- \| \| (1T tokens) \| |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| HellaSwag Original | 79.1 | *80.7* | 80.6 |
| HellaSwag Noise Avg. | 56.9 | 64.3 | *64.3* |
| \- AntSpeak | 45.6 | *61.3* | 57.9 |
| \- Drop | 53.8 | 57.3 | *58.2* |
| \- RandomCase | 55.3 | 65.0 | *65.7* |
| \- Repeat | 57.0 | 61.5 | *66.6* |
| \- UpperCase | 72.9 | 76.5 | *77.3* |
| Phonology-G2P | 11.8 | *18.9* | 13.0 |
| CUTE | 27.5 | 20.0 | *54.1* |
| \- Contains Char | 0.0 | 0.0 | *55.9* |
| \- Contains Word | 55.1 | 21.6 | *73.5* |
| \- Del Char | 34.6 | 34.3 | *35.9* |
| \- Del Word | 75.5 | *84.5* | 56.1 |
| \- Ins Char | 7.5 | 0.0 | *7.6* |
| \- Ins Word | 33.5 | *63.3* | 31.2 |
| \- Orthography | 43.1 | 0.0 | *52.4* |
| \- Semantic | 65 | 0.0 | *90.5* |
| \- Spelling | 1.1 | \- | *99.9* |
| \- Spelling Inverse | 30.1 | 3.6 | *99.9* |
| \- Substitute Char | 0.4 | 1.2 | *48.7* |
| \- Substitute Word | 16.4 | 6.8 | *72.8* |
| \- Swap Char | 2.6 | 2.4 | *11.5* |
| \- Swap Word | 20.1 | 4.1 | *21* |

Table 3: We compare our 8B B LT model to 8B BPE Llama 3 trained on 1T tokens on tasks that assess robustness to noise and awareness of the constituents of language (best result bold). We also report the performance of Llama 3.1 on the same tasks and underline best result overall. B LT outperforms the Llama 3 BPE model by a large margin and even improves over Llama 3.1 in many tasks indicating that the byte-level awareness is not something that can easily be obtained with more data.

We conduct a fixed inference scaling study to test the hypothesis that larger models taking fewer steps on larger patches might perform better than smaller models taking more steps. Starting from model sizes of 400m and 3.6B parameters with the Llama 2 tokenizer, we find flop equivalent models with the Llama 3 tokenizer and B LT-Entropy models with average patch sizes of 6 and 8 bytes on the training datamix (seeÂ  [Table 2](https://arxiv.org/html/2412.09871v1#S5.T2 "Table 2 â€£ Coding related generation tasks: â€£ 5.2 Beyond Compute Optimal Task Evaluations â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") for model details). For patch size 8 models, we use 3 encoder layers instead of 1. We train each model for various training flop budgets.

[Figure 1](https://arxiv.org/html/2412.09871v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") shows that B LT models achieve better scaling trends than tokenization-based architectures for both inference flop classes. In both cases, BPE models perform better with small training budgets and are quickly surpassed by B LT, not far beyond the compute-optimal regime. In practice, it can be preferable to spend more during the one-time pretraining to achieve a better performing model with a fixed inference budget. A perfect example of this is the class of 8B models, like Llama 3.1, which has been trained on two orders of magnitude more data than what is compute-optimal for that model size.

The crossover point where B LT improves over token-based models has shifted slightly closer to the compute-optimal point when moving to the larger flop class models (from 3x down to 2.5x the compute optimal budget). Similarly, the larger patch size 8 model has steeper scaling trend in the larger flop class overtaking the other models sooner. As discussed in SectionÂ  [5.1](https://arxiv.org/html/2412.09871v1#S5.SS1 "5.1 Parameter Matched Compute Optimal Scaling Trends â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), larger patch sizes appear to perform closer to BPE models at larger model scales. We attribute this, in part, to the decreasing share of total flop s used by the byte-level Encoder and Decoder modules which seem to scale slower than the Latent Transformer. When growing total parameters 20x from 400M to 8B, we only roughly double B LTâ€™s local model parameters. This is important as larger patch sizes only affect flop s from the patch Latent Transformer and not the byte-level modules. In fact, that is why the B LT-Entropy ps=8 went from 1.6x to 1.7x of the Llama 2 model size when moving to the larger model scale.

In summary, our patch-length scaling study demonstrates that the B LT patch-based architecture can achieve better scaling trends by simultaneously increasing both patch and model size. Such trends seem to persist and even improve at larger model scales.

<table><tbody><tr><th>Language</th><td colspan="2">Language <math><semantics><mo>â†’</mo> <annotation-xml><ci>â†’</ci></annotation-xml> <annotation>\rightarrow</annotation> <annotation>â†’</annotation></semantics></math> English</td><td colspan="2">English <math><semantics><mo>â†’</mo> <annotation-xml><ci>â†’</ci></annotation-xml> <annotation>\rightarrow</annotation> <annotation>â†’</annotation></semantics></math> Language</td></tr><tr><th></th><td>Llama 3</td><td><span>B</span> LT</td><td>Llama 3</td><td><span>B</span> LT</td></tr><tr><th><span>Arabic</span></th><td>22.3</td><td>24.6</td><td>10.4</td><td>8.8</td></tr><tr><th><span>German</span></th><td>41.3</td><td>42.0</td><td>29.8</td><td>31.2</td></tr><tr><th><span>Hindi</span></th><td>20.7</td><td>20.9</td><td>7.8</td><td>7.2</td></tr><tr><th><span>Italian</span></th><td>34.0</td><td>33.9</td><td>24.4</td><td>26.2</td></tr><tr><th><span>Vietnamese</span></th><td>31.2</td><td>31.0</td><td>28.4</td><td>23.7</td></tr><tr><th><span>Thai</span></th><td>17.9</td><td>18.1</td><td>10.5</td><td>7.7</td></tr><tr><th><span>Armenian</span></th><td>1.7</td><td>6.3</td><td>0.6</td><td>0.9</td></tr><tr><th><span>Amharic</span></th><td>1.3</td><td>3.1</td><td>0.4</td><td>0.5</td></tr><tr><th><span>Assamese</span></th><td>2.7</td><td>5.4</td><td>0.8</td><td>1.6</td></tr><tr><th><span>Bengali</span></th><td>4.7</td><td>12.7</td><td>1.7</td><td>4.1</td></tr><tr><th><span>Bosnian</span></th><td>36.0</td><td>37.3</td><td>16.9</td><td>19.6</td></tr><tr><th><span>Cebuano</span></th><td>18.2</td><td>20.6</td><td>5.8</td><td>9.1</td></tr><tr><th><span>Georgian</span></th><td>1.7</td><td>7.4</td><td>1.0</td><td>2.5</td></tr><tr><th><span>Gujarati</span></th><td>2.0</td><td>5.8</td><td>1.0</td><td>2.2</td></tr><tr><th><span>Hausa</span></th><td>5.75</td><td>5.9</td><td>1.2</td><td>1.3</td></tr><tr><th><span>Icelandic</span></th><td>16.1</td><td>17.9</td><td>4.8</td><td>5.3</td></tr><tr><th><span>Kannada</span></th><td>1.6</td><td>3.9</td><td>0.7</td><td>1.7</td></tr><tr><th><span>Kazakh</span></th><td>5.6</td><td>7.0</td><td>1.0</td><td>2.6</td></tr><tr><th><span>Kabuverdianu</span></th><td>20.3</td><td>20.9</td><td>5.1</td><td>6.8</td></tr><tr><th><span>Khmer</span></th><td>4.4</td><td>9.5</td><td>0.8</td><td>0.8</td></tr><tr><th><span>Kyrgyz</span></th><td>4.6</td><td>5.1</td><td>0.9</td><td>2.0</td></tr><tr><th><span>Malayalam</span></th><td>1.8</td><td>3.5</td><td>0.7</td><td>1.4</td></tr><tr><th><span>Odia</span></th><td>1.6</td><td>2.7</td><td>0.8</td><td>1.1</td></tr><tr><th><span>Somali</span></th><td>5.0</td><td>5.0</td><td>1.1</td><td>1.4</td></tr><tr><th><span>Swahili</span></th><td>10.1</td><td>12.0</td><td>1.4</td><td>2.3</td></tr><tr><th><span>Urdu</span></th><td>9.3</td><td>9.5</td><td>2.0</td><td>1.4</td></tr><tr><th><span>Zulu</span></th><td>4.7</td><td>5.0</td><td>0.6</td><td>0.5</td></tr><tr><th><span>Overall Average</span></th><td>12.1</td><td><span>14.0</span></td><td>5.9</td><td><span>6.4</span></td></tr></tbody></table>

Table 4: [^17]

## 6 Byte Modeling Improves Robustness

We also measure the robustness of B LT compared to token-based models that lack direct byte-level information, and present an approach to byte-ify pretrained token-based models.

### 6.1 Character-Level Tasks

A very early motivation for training byte-level models was to take advantage of their robustness to byte level noise in the input, and also to exploit their awareness of the constituents of tokens, which current tokenizer-based models struggle with. To measure these phenomena, we perform additional evaluations on benchmarks that evaluate both robustness to input noise as well as awareness of characters, both English and multi-lingual, including digits and phonemes. We present these results in Table [3](https://arxiv.org/html/2412.09871v1#S5.T3 "Table 3 â€£ 5.3 Patches Scale Better Than Tokens â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens").

##### Noisy Data

We create noised versions of the benchmark classification tasks described in Section [5.2](https://arxiv.org/html/2412.09871v1#S5.SS2 "5.2 Beyond Compute Optimal Task Evaluations â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), to compare the robustness of tokenizer-based models with that of B LT. We employ five distinct character-level noising strategies to introduce variations in the text: (a) AntSpeak: This strategy converts the entire text into uppercase, space-separated characters. (b) Drop: Randomly removes 10% of the characters from the text. (c) RandomCase: Converts 50% of the characters to uppercase and 50% to lowercase randomly throughout the text. (d) Repeat: Repeats 20% of the characters up to a maximum of four times. (e) UpperCase: Transforms all characters in the text to uppercase. During evaluation, we apply each noising strategy to either the prompt, completion, or both as separate tasks and report the average scores. In Table [3](https://arxiv.org/html/2412.09871v1#S5.T3 "Table 3 â€£ 5.3 Patches Scale Better Than Tokens â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") we report results on noised HellaSwag [^49] and find that B LT indeed outperforms tokenizer-based models across the board in terms of robustness, with an average advantage of 8 points over the model trained on the same data, and even improves over the Llama 3.1 model trained on a much larger dataset.

##### Phonology - Grapheme-to-Phoneme (G2P)

We assess B LTâ€™s capability to map a sequence of graphemes (characters representing a word) into a transcription of that wordâ€™s pronunciation (phonemes). In Table [3](https://arxiv.org/html/2412.09871v1#S5.T3 "Table 3 â€£ 5.3 Patches Scale Better Than Tokens â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we present the results of the G2P task in a 5-shot setting using Phonology Bench [^42] and find that B LT outperforms the baseline Llama 3 1T tokenizer-based model on this task.

| Task | Prompt | Llama 3 | B LT |
| --- | --- | --- | --- |
| Substitute Word | Question: Substitute " and " with " internet " in " She went to the kitchen and saw two cereals. ". Answer: | She went to the kitchen and saw two cereals. | She went to the kitchen internet saw two cereals. |
| Swap Char | Question: Swap " h " and " a " in " that ". Answer: | that | taht |
| Substitute Char | Question: Substitute " a " with " m " in " page ". Answer: | \- | pmge |
| Semantic Similarity | Question: More semantically related to " are ": " seem ", " acre ". Answer: | acre | seem |
| Orthographic Similarity | Question: Closer in Levenshtein distance to " time ": " timber ", " period ". Answer: | period | timber |
| Insert Char | Question: Add an " z " after every " n " in " not ". Answer: | znotz | nzot |

Figure 7: Output responses from Llama 3 and B LT models for various tasks from CUTE benchmark. B LT model performs better on sequence manipulation tasks compared to the tokenizer-based Llama 3 model. Note that few-shot examples are not shown in the above prompts to maintain clarity.

##### CUTE

To assess character-level understanding, we evaluate B LT on the CUTE benchmark [^14], which comprises several tasks that are broadly classified into three categories: understanding composition, understanding orthographic similarity, and ability to manipulate sequences. This benchmark poses a significant challenge for most tokenizer-based models, as they appear to possess knowledge of their tokensâ€™ spellings but struggle to effectively utilize this information to manipulate text. Table [3](https://arxiv.org/html/2412.09871v1#S5.T3 "Table 3 â€£ 5.3 Patches Scale Better Than Tokens â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") shows that B LT-Entropy outperforms both BPE Llama 3 models by more than 25 points on this benchmark. In particular, our model demonstrates exceptional proficiency in character manipulation tasks achieving 99.9% on both spelling tasks. Such large improvements despite B LT having been trained on 16x less data than Llama 3.1 indicates that character level information is hard to learn for BPE models. Figure [7](https://arxiv.org/html/2412.09871v1#S6.F7 "Figure 7 â€£ Phonology - Grapheme-to-Phoneme (G2P) â€£ 6.1 Character-Level Tasks â€£ 6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") illustrates a few such scenarios where Llama 3 tokenizer model struggles but our B LT model performs well. Word deletion and insertion are the only two tasks where BPE performs better. Such word manipulation might not be straightforward for a byte-level model but the gap is not too wide and building from characters to words could be easier than the other way around. We use the same evaluation setup in all tasks and the original prompts from Huggingface. BPE models might benefit from additional prompt engineering.

##### Low Resource Machine Translation

We evaluate B LT on translating into and out of six popular language families and twenty one lower resource languages with various scripts from the FLORES-101 benchmark [^17] and report SentencePiece BLEU in Table [4](https://arxiv.org/html/2412.09871v1#S5.T4 "Table 4 â€£ 5.3 Patches Scale Better Than Tokens â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). Our results demonstrate that B LT outperforms a model trained with the Llama 3 tokenizer, achieving a 2-point overall advantage in translating into English and a 0.5-point advantage in translating from English. In popular language pairs, B LT performs comparably to or slightly better than Llama 3. However, B LT outperforms Llama 3 on numerous language pairs within lower-resource language families, underscoring the effectiveness of byte modeling for generalizing to long-tail byte sequences.

|  | \| Llama 3 \| \| --- \| \| 8B \| \| (220B tokens) \| | \| B LT \| \| --- \| \| 8B \| \| (220B tokens) \| | \| B LT from Llama 3.1 \| \| --- \| \| 8B \| \| (220B tokens) \| | \| Llama 3.1 \| \| --- \| \| 8B \| \| (15T tokens) \| |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Arc-E | 67.4 | 66.8 | 66.6 | 83.4 |
| Arc-C | 40.4 | 38.8 | 45.8 | 55.2 |
| HellaSwag | 71.2 | 72.2 | 76.1 | 80.7 |
| PIQA | 77.0 | 78.2 | 77.4 | 80.7 |
| MMLU | 26.5 | 25.2 | 63.7 | 66.3 |
| MBPP | 11.8 | 10.0 | 38.2 | 47.2 |
| HumanEval | 9.2 | 7.3 | 34.2 | 37.2 |

Table 5: Initializing the global transformer model of B LT from the non-embedding parameters of Llama 3 improves performance on several benchmark tasks. First three models trained on the Llama 2 data for compute-optimal steps.

### 6.2 Training BLT from Llama 3

We explore a workflow where B LT models can leverage existing pre-trained tokenizer-based models for better and faster training convergence, acheived by initializing the global transformer parameters of B LT with those of a pre-trained Llama 3.1 model. Subsequently, we update the weights of the global transformer using one-tenth the learning rate employed for the local encoder and local decoder model, for Llama 3 optimal number of steps, and present a comparison with a baseline B LT in TableÂ  [5](https://arxiv.org/html/2412.09871v1#S6.T5 "Table 5 â€£ Low Resource Machine Translation â€£ 6.1 Character-Level Tasks â€£ 6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). It is evident that B LT from Llama 3.1 significantly outperforms both the Llama 3 and B LT baselines, which were trained with the same number of flop s. Moreover, when compared to our B LT-Entropy model (as presented in TableÂ  [1](https://arxiv.org/html/2412.09871v1#S5.T1 "Table 1 â€£ Coding related generation tasks: â€£ 5.2 Beyond Compute Optimal Task Evaluations â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")), which was trained on a significantly larger dataset (1T tokens), B LT from Llama 3.1 still achieves superior performance on MMLU task, suggesting that it can be an effective approach in significantly reducing the training flop s.

This setup can also be viewed as transforming tokenizer-based models into tokenizer-free ones, effectively converting a pre-trained LLaMA 3.1 model into a B LT model. To provide a comprehensive comparison, we include the original LLaMA 3.1 model trained on 15T tokens in Table [5](https://arxiv.org/html/2412.09871v1#S6.T5 "Table 5 â€£ Low Resource Machine Translation â€£ 6.1 Character-Level Tasks â€£ 6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") and evaluate it against the B LT derived from LLaMA 3. Our model experiences a slight performance decline on MMLU and HumanEval, but a more significant drop on other tasks. This suggests that further work is needed to fully leverage the pre-trained model and improve upon its performance, particularly in terms of optimizing data mixtures and other hyperparameters.

## 7 Ablations and Discussion

In this section, we discuss ablations justifying architectural choices for B LT and the patching scheme and hyper-parameters for the B LT 8B parameter model trained on the B LT-1T dataset.

![Refer to caption](https://arxiv.org/html/x8.png)

Figure 8: Variation of language modeling performance in bits-per-byte (bpb) with training flop s for 400m and 1b B LT models patched with entropy models of different sizes and context windows. Both dimensions improve scaling performance, with diminishing returns beyond 50m parameter entropy models with a context of 512 bytes.

##### Entropy Model Hyper-parameters

To study the effect of varying entropy model size and context window length on scaling performance, we train byte-level entropy transformer models of different model sizes between 1m and 100m parameters, with varying context window lengths from 64 to 512. We plot bpb vs training flop scaling law curves, created using our $400m$ and $1b$ B LT models trained on the Llama-2 dataset and present them in [Figure 8](https://arxiv.org/html/2412.09871v1#S7.F8 "Figure 8 â€£ 7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). We find that scaling performance is positively correlated with both these dimensions of the entropy model, with diminishing returns when we scale beyond 50m parameters.

##### Types of Patching

We ablate the four different patching schemes, introduced in Section [2](https://arxiv.org/html/2412.09871v1#S2 "2 Patching: From Individual Bytes to Groups of Bytes â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") i.e. 1) Strided Patching with a stride of 4 and 6, 2) Patching on whitepsace, 3) BPE Tokenizer patching based on the Llama 3 tokenizer, and 4) Entropy based patching using a small byte llm.

|  | \| Llama 3 \| \| --- \| \| BPE \| | \| Space Patching \| \| --- \| \| B LT \| | \| Entropy \| \| --- \| \| B LT \| |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Arc-E | 67.4 | 67.2 | 68.9 |
| Arc-C | 40.5 | 37.6 | 38.3 |
| HellaSwag | 71.3 | 70.8 | 72.7 |
| PIQA | 77.0 | 76.5 | 77.6 |

Table 6: [^13]

While dynamic patching reduces the effective length of sequences, we control for the sequence length to maintain a similar context length for all patching schemes. All the models see the same number of bytes in each sequence during training and inference in expectation to prevent any confounding factors from being able to model larger contexts. Figure [6](https://arxiv.org/html/2412.09871v1#S5.F6 "Figure 6 â€£ 5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") highlights the results of these ablations. All the remaining patching schemes outperform static patching, with space patching being a very close competitor to dynamic entropy based patching.

In [Table 6](https://arxiv.org/html/2412.09871v1#S7.T6 "Table 6 â€£ Types of Patching â€£ 7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we present benchmark evaluations for B LT models comparing tokenizer-based models, space patching, and entropy-based patching, trained on the Llama 2 dataset for an optimal number of steps [^13]. Although space patching is a simpler strategy that does not involve running an entropy model on the fly during training, we find that the gains we observed using entropy-based patching on scaling trends (Section [5](https://arxiv.org/html/2412.09871v1#S5 "5 Scaling Trends â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")) do indeed carry forward even to downstream benchmark tasks.<sup>7</sup> <sup>7</sup> 7 Space patching results are from earlier runs without cross-attention, but similar trends are observed even with cross-attention.

<table><thead><tr><th></th><th></th><th></th><th colspan="4">BPB</th></tr><tr><th>Cross Attn. Dec.</th><th>Cross Attn. Enc.</th><th>Pooling Init</th><th>Wikipedia</th><th>CC</th><th>Github</th><th>Train Dist</th></tr></thead><tbody><tr><th>-</th><th>All Layers</th><th>False</th><td>0.830</td><td>0.915</td><td><span>0.442</span></td><td>0.891</td></tr><tr><th>-</th><th>Last Layer</th><th>False</th><td>0.836</td><td>0.906</td><td>0.447</td><td>0.886</td></tr><tr><th>-</th><th>-</th><th>-</th><td>0.833</td><td>0.892</td><td>0.446</td><td>0.866</td></tr><tr><th>First Layer</th><th>Last Layer</th><th>True</th><td>0.825</td><td>0.883</td><td>0.443</td><td>0.861</td></tr><tr><th>All Layers</th><th>Last Layer</th><th>True</th><td><span>0.823</span></td><td>0.871</td><td>0.443</td><td>0.846</td></tr><tr><th>All Layers</th><th>All Layers</th><th>True</th><td>0.828</td><td><span>0.868</span></td><td>0.443</td><td><span>0.844</span></td></tr></tbody></table>

Table 7: Ablations on the use of Cross Attention for a 1B B LT model trained on 100B bytes. We report bits-per-byte (bpb) on different datasets. We also report bpb on a random sample of the training data (denoted as Train Dist.) The Cross Attn. Enc. and Dec. columns denote which transformer layers the cross-attention block is applied after (or before for the decoder) in the local encoder and decoder respectively.

<table><thead><tr><th></th><th></th><th></th><th colspan="4">BPB</th></tr><tr><th>Ngram Sizes</th><th>Per Ngram Vocab</th><th>Total Vocab</th><th>Wikipedia</th><th>CC</th><th>Github</th><th>Train Dist</th></tr></thead><tbody><tr><th>-</th><th>-</th><th>-</th><td>0.892</td><td>0.867</td><td>0.506</td><td>0.850</td></tr><tr><th>6,7,8</th><th>100k</th><th>300k</th><td>0.873</td><td>0.860</td><td>0.499</td><td>0.842</td></tr><tr><th>6,7,8</th><th>200k</th><th>600k</th><td>0.862</td><td>0.856</td><td>0.492</td><td>0.838</td></tr><tr><th>3,4,5</th><th>100k</th><th>300k</th><td>0.859</td><td>0.855</td><td>0.491</td><td>0.837</td></tr><tr><th>6,7,8</th><th>400k</th><th>1M</th><td>0.855</td><td>0.853</td><td>0.491</td><td>0.834</td></tr><tr><th>3,4,5</th><th>200k</th><th>600k</th><td>0.850</td><td>0.852</td><td>0.485</td><td>0.833</td></tr><tr><th>3,4,5,6,7,8</th><th>100k</th><th>600k</th><td>0.850</td><td>0.852</td><td>0.486</td><td>0.833</td></tr><tr><th>3,4,5</th><th>400k</th><th>1M</th><td>0.844</td><td>0.851</td><td>0.483</td><td>0.832</td></tr><tr><th>3,4,5,6,7,8</th><th>200k</th><th>1M</th><td>0.840</td><td>0.849</td><td>0.481</td><td>0.830</td></tr><tr><th>3,4,5,6,7,8</th><th>400k</th><th>2M</th><td><span>0.831</span></td><td><span>0.846</span></td><td><span>0.478</span></td><td><span>0.826</span></td></tr></tbody></table>

Table 8: Ablations on the use of n-gram hash embedding tables for a 1B B LT model trained on 100B bytes. We find that hash n-gram embeddings are very effective with very large improvements in BPB. The most significant parameter is the per-ngram vocab size and that smaller ngram sizes are more impactful than larger ones.

##### Cross-Attention

In [Table 7](https://arxiv.org/html/2412.09871v1#S7.T7 "Table 7 â€£ Types of Patching â€£ 7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we ablate including cross-attention at various points in the encoder and decoder of B LT. For the encoder cross-attention we test initializing the queries with 1) the same learned embedding for every global state, 2) a hash embedding of the bytes in the patch, and 3) pooling of the encoder hidden representation of the patch bytes at the given encoder layer.

We find that using cross-attention in the decoder is most effective. In the encoder, there is a slight improvement in using cross-attention but only with pooling initialization of queries. Additionally, we find that cross-attention helps particularly on Common-Crawl and especially with larger patch sizes.

##### n-gram Hash Embeddings

We ablate settings of 0, 100K, 200K and 400K n-gram hash embedding vocabularies and present results in Table [8](https://arxiv.org/html/2412.09871v1#S7.T8 "Table 8 â€£ Types of Patching â€£ 7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). We find that hash embeddings help on all domains, but particularly on Wikipedia and Github (0.04 bpb difference compared to 0.01 bpb difference after 15k steps at 8B). At 8B scale going from 500K to 300K hashes changed performance by Â 0.001 bpb on 15k steps. This indicates that hashes are vital to bringing the performance of B LT to match those of tokenizer based models, however, after 300K hashes, there are diminishing returns. Additionally, it appears that the gains are largely complementary with cross-attention as they provide improvements on different datasets.

| Ngram Embeddings | Encoder Layers | Decoder Layers | Train Dist BPB |
| --- | --- | --- | --- |
| False | 1 | 9 | 0.850 |
| False | 5 | 5 | 0.843 |
| True | 5 | 5 | 0.844 |
| True | 3 | 7 | 0.824 |
| True | 1 | 9 | 0.822 |

Table 9: When paired with hash n-gram embeddings, a light-weight local encoder is sufficient. More layers can then be allocated to the decoder for the same cost.

##### Local Model Hyperparamaters

In Table [9](https://arxiv.org/html/2412.09871v1#S7.T9 "Table 9 â€£ n-gram Hash Embeddings â€£ 7 Ablations and Discussion â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we ablate various settings for the number of layers in the local encoder and decoder. When paired with hash n-gram embeddings, B LT works well with an encoder that is extremely light-weight i.e. just one layer, and with a heavier decoder.

## 8 Related Work

Character-Level RNNs: Character Language Modeling has been a popular task ever since the early days of neural models [^41] owing to their flexibility of modeling out of vocabulary words organically without resorting to back-off methods. [^26] also train a model that processes characters only on the input side using convolutional and highway networks that feed into LSTM-based RNNs and are able to match performance with the RNN based state-of-the-art language models of the time on English and outperform them on morphologically rich languages, another sought-after advantage of character-level LLMs. [^25] do machine comprehension using byte-level LSTM models that outperformed word-level models again on morphologically-rich Turkish and Russian languages. Along similar lines, [^51] used character-based convolutional models for classification tasks, which outperformed word-level models for certain tasks. [^8] use hierarchical LSTM models using boundary-detectors at each level to discover the latent hierarchy in text, to further improve performance on character level language modeling. ByteNet by [^23] uses CNN based layers on characters as opposed to attention for machine translation.

Character-Level Transformers: The development of transformer models using attention [^44] together with subword tokenization [^37], significantly improved the performance of neural models on language modeling and benchmark tasks. However, word and sub-word units implicitly define an inductive bias for the level of abstraction models should operate on. To combine the successes of transformer models with the initial promising results on character language modeling, [^1] use very deep transformers, and with the help of auxiliary losses, train transformer-based models that outperformed previous LSTM based character llm s. However, they still saw a significant gap from word level LLMs. GPT-2 [^36] also observed that on large scale datasets like the 1 billion word benchmark, byte-level LMs were not competitive with word-level LMs.

While [^7] demonstrated that byte-level llm s based on transformers can outperform subword level LLMs with comparable parameters, the models take up much more compute and take much longer to train. Similarly, [^15] train a BERT model (CharFormer) that builds word representations by applying convolutions on character embeddings, and demonstrate improvements on the medical domain, but they also expend much more compute in doing so. [^9] develop CANINE, a 150M parameter encoder-only model that operates directly on character sequences. CANINE uses a deep transformer stack at its core similar in spirit to our global model, and a combination of a local transformer and strided convolutions to downsample the input characters, and outperforms the equivalent token-level encoder-only model (mBERT) on downstream multilingual tasks. ByT5 [^47] explored approaches for byte-level encoder decoder models, that do not use any kind of patching operations. While their model exhibited improved robustness to noise, and was competitive with tokenizer-based models with 4x less data, the lack of patching meant that the models needed to compute expensive attention operations over every byte, which was extremely compute heavy. Directly modeling bytes instead of subword units increases the sequence length of the input making it challenging to efficiently scale byte level models. Recently, using the Mamba Architecture [^19], which can maintain a fixed-size memory state over a very large context length, [^45] train a byte-level Mamba architecture also without using patching, and are able to outperform byte-level transformer models in a flop controlled setting at the 350M parameter scale in terms of bits-per-byte on several datasets.

Patching-based approaches: The effective use of patching can bring down the otherwise inflated number of flop s expended by byte-level LLMs while potentially retaining performance, and many works demonstrated initial successes at a small scale of model size and number of training bytes. [^33] experiment with static patching based downsampling and upsampling and develop the hourglass transformer which outperforms other byte-level baselines at the 150M scale. [^34] further improve this with the help of dynamic patching schemes, including a boundary-predictor that is learned in an end-to-end fashion, a boundary-predictor supervised using certain tokenizers, as well as an entropy-based patching model similar to B LT, and show that this approach can outperform the vanilla transformers of the time on language modeling tasks at a 40M parameter scale on Â 400M tokens. [^27] investigate training on sequences compressed using arithmetic coding to achieve compression rates beyond what BPE can achieve, and by using a equal-info windows technique, are able to outperform byte-level baselines on language modeling tasks, but underperform subword baselines.

Our work draws inspiration and is most closely related to MegaByte [^48], which is a decoder only causal LLM that uses a fixed static patching and concatenation of representations to convert bytes to patches, and uses a local model on the decoder side to convert from patches back into bytes. They demonstrate that MegaByte can match tokenizer-based models at a 1B parameter scale on a dataset of 400B bytes. We ablate MegaByte in all our experiments and find that static patching lags behind the current state-of-the-art compute optimally trained tokenizer based models in a flop controlled setting and we demonstrate how B LT bridges this gap. [^39] make the same observation about MegaByte and suggest extending the static patching method to patching on whitespaces and other space-like bytes, and also add a local encoder model. They find improvements over tokenized-based transformer models in a compute controlled setting on some domains such as Github and arXiv at the 1B parameter scale. We also report experiments with this model, and show that further architectural improvements are needed to scale up byte-level models even further and truly match current state-of-the-art token-based models such as Llama 3.

## 9 Limitations and Future Work

In this work, for the purposes of architectural choices, we train models for the optimal number of steps as determined for Llama 3 [^13]. However, these scaling laws were calculated for BPE-level transformers and may lead to suboptimal (data, parameter sizes) ratios in the case of B LT. We leave for future work the calculation of scaling laws for B LT potentially leading to even more favorable scaling trends for our architecture. Additionally, many of these experiments were conducted at scales upto 1B parameters, and it is possible for the optimal architectural choices to change as we scale to 8B parameters and beyond, which may unlock improved performance for larger scales.

Existing transformer libraries and codebases are designed to be highly efficient for tokenizer-based transformer architectures. While we present theoretical flop matched experiments and also use certain efficient implementations (such as FlexAttention) to handle layers that deviate from the vanilla transformer architecture, our implementations may yet not be at parity with tokenizer-based models in terms of wall-clock time and may benefit from further optimizations.

While B LT uses a separately trained entropy model for patching, learning the patching model in an end-to-end fashion can be an interesting direction for future work. In Section [6.2](https://arxiv.org/html/2412.09871v1#S6.SS2 "6.2 Training BLT from Llama 3 â€£ 6 Byte Modeling Improves Robustness â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"), we present initial experiments showing indications of success for â€œbyte-ifyingâ€ tokenizer-based models such as Llama 3 that are trained on more than 10T tokens, by initializing and freezing the global transformer with their weights. Further work in this direction may uncover methods that not only retain the benefits of bytefying, but also push performance beyond that of these tokenizer-based models without training them from scratch.

## 10 Conclusion

This paper presents the Byte Latent Transformer (BLT), a new architecture that redefines the conventional dependency on fixed-vocabulary tokenization in large language models. By introducing a dynamic, learnable method for grouping bytes into patches, B LT effectively allocates computational resources based on data complexity, leading to significant improvements in both efficiency and robustness. Our extensive scaling study demonstrates that B LT models can match the performance of tokenization-based models like Llama 3 at scales up to 8B and 4T bytes, and can trade minor losses in evaluation metrics for up to 50% reductions in inference flop s. Furthermore, B LT unlocks a new dimension for scaling, allowing simultaneous increases in model and patch size within a fixed inference budget. This new paradigm becomes advantageous for compute regimes commonly encountered in practical settings. While directly engaging with raw byte data, B LT also improves the modelâ€™s ability to handle the long-tail of data, offering significant improvements in robustness to noisy inputs and a deeper understanding of sub-word structures. Overall, these results position B LT as a promising alternative to traditional tokenization-based approaches, providing a scalable and robust framework for more efficient and adaptable language models.

## Acknowledgements

We would like to thank Kalyan Saladi for help with everything relating to pre-training infrastructure; Gabriel Synnaeve, Ammar Rizvi, Jacob Kahn, Michel Meyer for helping organize resources for scaling up B LT; Badr Youbi Idirissi, Mathurin Videau, and Jade Copet for invaluable discussions and feedback about B LT, for access to the Lingua framework for open-sourcing code for B LT, and for help preparing the B LT-1T dataset used in this paper; Omer Levy, who was actively involved in the early stages of the project and provided valuable feedback and ideas; Driss Guessous for help with FlexAttention; and Sida Wang, Melanie Sclar, Amanda Bertsch, and Hunter Lang for feedback and discussions.

## Contributors

In this section, we list individual contributions.

##### Core Contributors:

Artidoro Pagnoni, Srinivasan Iyer, Ramakanth Pasunuru, Pedro Rodriguez, John Nguyen, Gargi Ghosh (Project Lead)

##### Core Advising Group:

Mike Lewis, Ari Holtzman, Luke Zettlemoyer

##### Advisors and Contributors:

Jason Weston, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu

## References

## 11 Model Hyper Parameters

Table [10](https://arxiv.org/html/2412.09871v1#S11.T10 "Table 10 â€£ 11 Model Hyper Parameters â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") shows different hyper parameter settings for B LT models.

<table><tbody><tr><td></td><td></td><td colspan="4">Encoder</td><td colspan="4">Global Latent Transf.</td><td colspan="4">Decoder</td><td colspan="2">Cross-Attn.</td></tr><tr><td>Model</td><td></td><td><math><semantics><msub><mi>l</mi> <mi>â„°</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ‘™</ci> <ci>â„°</ci></apply></annotation-xml> <annotation>l_{\mathcal{E}}</annotation> <annotation>italic_l start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT</annotation></semantics></math></td><td>#heads</td><td><math><semantics><msub><mi>h</mi> <mi>â„°</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>â„</ci> <ci>â„°</ci></apply></annotation-xml> <annotation>h_{\mathcal{E}}</annotation> <annotation>italic_h start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT</annotation></semantics></math></td><td>#Params</td><td><math><semantics><msub><mi>l</mi> <mi>ğ’¢</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ‘™</ci> <ci>ğ’¢</ci></apply></annotation-xml> <annotation>l_{\mathcal{G}}</annotation> <annotation>italic_l start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT</annotation></semantics></math></td><td>#heads</td><td><math><semantics><msub><mi>h</mi> <mi>ğ’¢</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>â„</ci> <ci>ğ’¢</ci></apply></annotation-xml> <annotation>h_{\mathcal{G}}</annotation> <annotation>italic_h start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT</annotation></semantics></math></td><td>#Params</td><td><math><semantics><msub><mi>l</mi> <mi>ğ’Ÿ</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>ğ‘™</ci> <ci>ğ’Ÿ</ci></apply></annotation-xml> <annotation>l_{\mathcal{D}}</annotation> <annotation>italic_l start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT</annotation></semantics></math></td><td>#heads</td><td><math><semantics><msub><mi>h</mi> <mi>ğ’Ÿ</mi></msub> <annotation-xml><apply><csymbol>subscript</csymbol> <ci>â„</ci> <ci>ğ’Ÿ</ci></apply></annotation-xml> <annotation>h_{\mathcal{D}}</annotation> <annotation>italic_h start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT</annotation></semantics></math></td><td>#Params</td><td>#heads</td><td>k</td></tr><tr><td><span>400M</span></td><td></td><td>1</td><td>12</td><td>768</td><td>7M</td><td>24</td><td>10</td><td>1280</td><td>470M</td><td>7</td><td>12</td><td>768</td><td>50M</td><td>10</td><td>2</td></tr><tr><td><span>1B</span></td><td></td><td>1</td><td>16</td><td>1024</td><td>12M</td><td>25</td><td>16</td><td>2048</td><td>1B</td><td>9</td><td>16</td><td>1024</td><td>113M</td><td>16</td><td>2</td></tr><tr><td><span>2B</span></td><td></td><td>1</td><td>16</td><td>1024</td><td>12M</td><td>26</td><td>20</td><td>2560</td><td>2B</td><td>9</td><td>16</td><td>1024</td><td>113M</td><td>16</td><td>3</td></tr><tr><td><span>4B</span></td><td></td><td>1</td><td>16</td><td>1024</td><td>12M</td><td>36</td><td>24</td><td>3072</td><td>4.1B</td><td>9</td><td>16</td><td>1024</td><td>113M</td><td>16</td><td>3</td></tr><tr><td><span>8B</span></td><td></td><td>1</td><td>20</td><td>1280</td><td>20M</td><td>32</td><td>32</td><td>4096</td><td>6.4B</td><td>6</td><td>20</td><td>1280</td><td>120M</td><td>20</td><td>4</td></tr></tbody></table>

Table 10: Architectural hyper-parameters for different B LT model sizes that we train for flop -controlled experiments described in this paper.

## 12 FLOPs Equations

Here, we provide the equations used for flop computation for the forward-pass of transformer and B LT models based on [^21]. We assume that the backward pass uses twice as much flop s as the forward pass.

| Operation | flop s per token/byte |
| --- | --- |
| Attention $(l,h_{k},n_{heads},m)$ | $4\times l\times h_{k}\times n_{heads}\times\frac{m+1}{2}$ |
| QKVO $(l,h,r)$ | $(r\times 2+2)\times 2\times l\times h^{2}$ |
| Feed-forward $(l,h,d_{ff})$ | $2\times l\times 2\times h\times d_{ff}h$ |
| De-Embedding $(h,V)$ | $2\times h\times\|V\|$ |
| Cross-Attention $(l,h_{k},n_{heads},p,r)$ | Attention $(l,h_{k},n_{heads},p)$ + QKVO $(l,h_{k}\times n_{heads},r)$ |

Table 11: flop s for operations used in transformer and B LT models. $l$ corresponds to layers, $h$ is the hidden dimension ( $h_{k}$ with $n_{heads}$ heads), $m$ is the context length, $d_{ff}=4$ is the feed-forward dimension multiplier, $p$ is the patch size, and $r$ is the ratio of queries to keys.

For a transformer model with $l$ layers, hidden dimension $h$ , context length $m$ , $n_{heads}$ attention heads of dimension $h_{k}$ , and a feed-forward multipler of $d_{ff}$ , we compute flop s as:

<table><tbody><tr><td></td><td><math><semantics><mrow><mtext>Transformer-FLOPs</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>l</mi><mo>,</mo><mi>h</mi><mo>,</mo><mi>m</mi><mo>,</mo><msub><mi>n</mi> <mrow><mi>h</mi> <mo>â¢</mo> <mi>e</mi> <mo>â¢</mo> <mi>a</mi> <mo>â¢</mo> <mi>d</mi> <mo>â¢</mo> <mi>s</mi></mrow></msub><mo>,</mo><msub><mi>h</mi> <mi>k</mi></msub><mo>,</mo><msub><mi>d</mi> <mrow><mi>f</mi> <mo>â¢</mo> <mi>f</mi></mrow></msub><mo>,</mo><mi>V</mi><mo>)</mo></mrow></mrow> <annotation-xml><apply><ci><mtext>Transformer-FLOPs</mtext></ci> <vector><ci>ğ‘™</ci> <ci>â„</ci> <ci>ğ‘š</ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘›</ci> <apply><ci>â„</ci> <ci>ğ‘’</ci> <ci>ğ‘</ci> <ci>ğ‘‘</ci> <ci>ğ‘ </ci></apply></apply> <apply><csymbol>subscript</csymbol> <ci>â„</ci> <ci>ğ‘˜</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘‘</ci> <apply><ci>ğ‘“</ci> <ci>ğ‘“</ci></apply></apply> <ci>ğ‘‰</ci></vector></apply></annotation-xml> <annotation>\displaystyle\text{Transformer-FLOPs}(l,h,m,n_{heads},h_{k},d_{ff},V)</annotation> <annotation>Transformer-FLOPs ( italic_l, italic_h, italic_m, italic_n start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d italic_s end_POSTSUBSCRIPT, italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT, italic_V )</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><mtext>Feed-forward</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>l</mi><mo>,</mo><mi>h</mi><mo>,</mo><msub><mi>d</mi> <mrow><mi>f</mi> <mo>â¢</mo> <mi>f</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><ci><mtext>Feed-forward</mtext></ci> <vector><ci>ğ‘™</ci> <ci>â„</ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘‘</ci> <apply><ci>ğ‘“</ci> <ci>ğ‘“</ci></apply></apply></vector></apply></apply></annotation-xml> <annotation>\displaystyle=\text{Feed-forward}(l,h,d_{ff})</annotation> <annotation>= Feed-forward ( italic_l, italic_h, italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(19)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mrow><mtext>QKVO</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mrow><mrow><mi>l</mi><mo>,</mo><mi>h</mi><mo>,</mo><mi>r</mi></mrow> <mo>=</mo> <mn>1</mn></mrow><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><apply><ci><mtext>QKVO</mtext></ci> <apply><list><ci>ğ‘™</ci> <ci>â„</ci> <ci>ğ‘Ÿ</ci></list> <cn>1</cn></apply></apply></apply></annotation-xml> <annotation>\displaystyle+\text{QKVO}(l,h,r=1)</annotation> <annotation>+ QKVO ( italic_l, italic_h, italic_r = 1 )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(20)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mrow><mtext>Attention</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>l</mi><mo>,</mo><msub><mi>h</mi> <mi>k</mi></msub><mo>,</mo><msub><mi>n</mi> <mrow><mi>h</mi> <mo>â¢</mo> <mi>e</mi> <mo>â¢</mo> <mi>a</mi> <mo>â¢</mo> <mi>d</mi> <mo>â¢</mo> <mi>s</mi></mrow></msub><mo>,</mo><mi>m</mi><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><apply><ci><mtext>Attention</mtext></ci> <vector><ci>ğ‘™</ci> <apply><csymbol>subscript</csymbol> <ci>â„</ci> <ci>ğ‘˜</ci></apply> <apply><csymbol>subscript</csymbol> <ci>ğ‘›</ci> <apply><ci>â„</ci> <ci>ğ‘’</ci> <ci>ğ‘</ci> <ci>ğ‘‘</ci> <ci>ğ‘ </ci></apply></apply> <ci>ğ‘š</ci></vector></apply></apply></annotation-xml> <annotation>\displaystyle+\text{Attention}(l,h_{k},n_{heads},m)</annotation> <annotation>+ Attention ( italic_l, italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, italic_n start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d italic_s end_POSTSUBSCRIPT, italic_m )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(21)</span></td></tr></tbody><tbody><tr><td></td><td></td><td><math><semantics><mrow><mo>+</mo> <mrow><mtext>De-Embedding</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>h</mi><mo>,</mo><mi>V</mi><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><apply><ci><mtext>De-Embedding</mtext></ci> <interval><ci>â„</ci> <ci>ğ‘‰</ci></interval></apply></apply></annotation-xml> <annotation>\displaystyle+\text{De-Embedding}(h,V)</annotation> <annotation>+ De-Embedding ( italic_h, italic_V )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(22)</span></td></tr></tbody></table>

For B LT models, we use the above-mentioned primitives together with the flop s equation from Section [4.5](https://arxiv.org/html/2412.09871v1#S4.SS5 "4.5 FLOPs Estimation â€£ 4 Experimental Setup â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") to compute total flop s.

## 13 Rolling Polynomial Hashing

Given a byte $n$ -gram $g_{i,n}=\{b_{i-n+1},\ldots,b_{i}\}$ , the rolling polynomial hash of $g_{i,n}$ is defined as:

<table><tbody><tr><td></td><td><math><semantics><mrow><mtext>Hash</mtext> <mo>â¢</mo> <mrow><mo>(</mo><msub><mi>g</mi> <mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow> <annotation-xml><apply><ci><mtext>Hash</mtext></ci> <apply><csymbol>subscript</csymbol> <ci>ğ‘”</ci> <list><ci>ğ‘–</ci> <ci>ğ‘›</ci></list></apply></apply></annotation-xml> <annotation>\displaystyle\text{Hash}(g_{i,n})</annotation> <annotation>Hash ( italic_g start_POSTSUBSCRIPT italic_i, italic_n end_POSTSUBSCRIPT )</annotation></semantics></math></td><td><math><semantics><mrow><mo>=</mo> <mrow><mstyle><munderover><mo>âˆ‘</mo> <mrow><mi>j</mi> <mo>=</mo> <mn>1</mn></mrow> <mi>n</mi></munderover></mstyle> <mrow><msub><mi>b</mi> <mrow><mrow><mi>i</mi> <mo>âˆ’</mo> <mi>j</mi></mrow> <mo>+</mo> <mn>1</mn></mrow></msub> <mo>â¢</mo> <msup><mi>a</mi> <mrow><mi>j</mi> <mo>âˆ’</mo> <mn>1</mn></mrow></msup></mrow></mrow></mrow> <annotation-xml><apply><csymbol>absent</csymbol> <apply><apply><csymbol>superscript</csymbol> <apply><csymbol>subscript</csymbol> <apply><ci>ğ‘—</ci> <cn>1</cn></apply></apply> <ci>ğ‘›</ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘</ci> <apply><apply><ci>ğ‘–</ci> <ci>ğ‘—</ci></apply> <cn>1</cn></apply></apply> <apply><csymbol>superscript</csymbol> <ci>ğ‘</ci> <apply><ci>ğ‘—</ci> <cn>1</cn></apply></apply></apply></apply></apply></annotation-xml> <annotation>\displaystyle=\sum_{j=1}^{n}b_{i-j+1}a^{j-1}</annotation> <annotation>= âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i - italic_j + 1 end_POSTSUBSCRIPT italic_a start_POSTSUPERSCRIPT italic_j - 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td><td></td><td rowspan="1"><span>(23)</span></td></tr></tbody></table>

Where $a$ is chosen to be a 10-digit prime number.

## 14 Frequency-based n-gram Embedddings

Prior to using hash n-gram embeddings in the final B LT architecture, we also experimented with frequency-based n-gram embeddings. For each $n\in\{1,2,3,4,5,6,7,8\}$ there is an embedding matrix $E_{n}^{ngram}$ that contains the most frequent byte-grams for the given $n$ . Since it is intractable to store embeddings as $n$ grows, we only store embeddings for the most frequent $100,000$ byte-grams for each byte-gram. If a particular position $i$ includes an $n$ -gram present in the corresponding the embedding matrix, then this embedding is passed to the next step, encoder multi-headed cross-attention. If a byte-gram is infrequent and therefore not in the matrix, then its embedding is obtained from encoder hash embeddings instead.

Since frequency-based $n$ -grams are limited by the vocabulary of the n-gram tables with infrequent n-grams not being represented at all, we subsequently moved to hash-based $n$ -gram embeddings. See [Table 12](https://arxiv.org/html/2412.09871v1#S14.T12 "Table 12 â€£ 14 Frequency-based n-gram Embedddings â€£ Byte Latent Transformer: Patches Scale Better Than Tokens") for a comparison of hash and frequency based n-gram embeddings.

<table><tbody><tr><td></td><th></th><th></th><th></th><th></th><th colspan="4">bpb</th></tr><tr><th>Hash Ngram Sizes</th><th>Per Hash Ngram Vocab</th><th>Ngram Sizes</th><th>Per Ngram Vocab</th><th>Total Vocab</th><th>Wikipedia</th><th>CC</th><th>Github</th><th>Train Dist</th></tr><tr><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.892</td><td>0.867</td><td>0.506</td><td>0.850</td></tr><tr><td>6,7,8</td><td>50k</td><td>6,7,8</td><td>50k</td><td>300k</td><td>0.878</td><td>0.860</td><td>0.497</td><td>0.843</td></tr><tr><td>6,7,8</td><td>100k</td><td>-</td><td>-</td><td>300k</td><td>0.873</td><td>0.860</td><td>0.499</td><td>0.842</td></tr><tr><td>6,7,8</td><td>100k</td><td>6,7,8</td><td>100k</td><td>600k</td><td>0.868</td><td>0.857</td><td>0.494</td><td>0.839</td></tr><tr><td>6,7,8</td><td>200k</td><td>-</td><td>-</td><td>600k</td><td>0.862</td><td>0.856</td><td>0.492</td><td>0.838</td></tr><tr><td>3,4,5</td><td>50k</td><td>3,4,5</td><td>50k</td><td>300k</td><td>0.862</td><td>0.856</td><td>0.491</td><td>0.837</td></tr><tr><td>3,4,5</td><td>100k</td><td>-</td><td>-</td><td>300k</td><td>0.859</td><td>0.855</td><td>0.491</td><td>0.837</td></tr><tr><td>6,7,8</td><td>200k</td><td>6,7,8</td><td>200k</td><td>1M</td><td>0.861</td><td>0.855</td><td>0.491</td><td>0.837</td></tr><tr><td>6,7,8</td><td>400k</td><td>-</td><td>-</td><td>1M</td><td>0.855</td><td>0.853</td><td>0.491</td><td>0.834</td></tr><tr><td>3,4,5,6,7,8</td><td>50k</td><td>3,4,5,6,7,8</td><td>50k</td><td>600k</td><td>0.855</td><td>0.853</td><td>0.488</td><td>0.834</td></tr><tr><td>3,4,5</td><td>100k</td><td>3,4,5</td><td>100k</td><td>600k</td><td>0.851</td><td>0.853</td><td>0.486</td><td>0.834</td></tr><tr><td>3,4,5</td><td>200k</td><td>-</td><td>-</td><td>600k</td><td>0.850</td><td>0.852</td><td>0.485</td><td>0.833</td></tr><tr><td>3,4,5,6,7,8</td><td>100k</td><td>-</td><td>-</td><td>600k</td><td>0.850</td><td>0.852</td><td>0.486</td><td>0.833</td></tr><tr><td>3,4,5</td><td>400k</td><td>-</td><td>-</td><td>1M</td><td>0.844</td><td>0.851</td><td>0.483</td><td>0.832</td></tr><tr><td>3,4,5</td><td>200k</td><td>3,4,5</td><td>200k</td><td>1M</td><td>0.843</td><td>0.850</td><td>0.482</td><td>0.830</td></tr><tr><td>3,4,5,6,7,8</td><td>100k</td><td>3,4,5,6,7,8</td><td>100k</td><td>1M</td><td>0.844</td><td>0.850</td><td>0.482</td><td>0.830</td></tr><tr><td>3,4,5,6,7,8</td><td>200k</td><td>-</td><td>-</td><td>1M</td><td>0.840</td><td>0.849</td><td>0.481</td><td>0.830</td></tr><tr><td>3,4,5,6,7,8</td><td>200k</td><td>3,4,5,6,7,8</td><td>200k</td><td>2M</td><td><span>0.833</span></td><td><span>0.846</span></td><td><span>0.478</span></td><td><span>0.826</span></td></tr><tr><td>3,4,5,6,7,8</td><td>400k</td><td>-</td><td>-</td><td>2M</td><td><span>0.831</span></td><td><span>0.846</span></td><td><span>0.478</span></td><td><span>0.826</span></td></tr></tbody></table>

Table 12: Ablations on the use of frequency-based as well as hash-based n-gram embedding tables for a 1B B LT model trained on 100B bytes.

## 15 Entropy Patching Example from MMLU

We illustrate how a few-shot example from a downstream task i.e. MMLU [^20], is patched using an entropy-model trained for use with B LT models in Figure [9](https://arxiv.org/html/2412.09871v1#S15.F9 "Figure 9 â€£ 15 Entropy Patching Example from MMLU â€£ Byte Latent Transformer: Patches Scale Better Than Tokens"). Directly using the entropy model with the full-context window causes repetitive patterns to be heavily patched. For example, â€œ10 times, with an rms deviation of aboutâ€ in the MMLU query is patched frequently the first time it is encountered, but is part of very large patches the next three times, which, although inference efficient, maybe undesirable for reasoning. One method that we use to avoid such a â€œentropyâ€ drift is by resetting the entropy context with new lines and using a approximate monotonicity constraint (see Section [4.4](https://arxiv.org/html/2412.09871v1#S4.SS4 "4.4 Entropy Model Context â€£ 4 Experimental Setup â€£ Byte Latent Transformer: Patches Scale Better Than Tokens")).

![Refer to caption](https://arxiv.org/html/extracted/6066458/assets/patching.png)

Figure 9: An example of default entropy-based patching with global threshold during inference on mmlu. Green denotes the prompt, Blue denotes the few-shot examples, and red denotes the question to be answered. Note that the size of the patches for the repeated phrases in the answer choices is much larger, which means that the global model is invoked significantly fewer times than its tokenizer-based counterpart, with this inference patching scheme.

[^1]: Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.Character-level language modeling with deeper self-attention.In *Association for the Advancement of Artificial Intelligence*, volume 33, pages 3159â€“3166, 2019.

[^2]: Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.Program synthesis with large language models, 2021.

[^3]: Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and Kilian Weinberger.Learning to rank with (a lot of) word features.*Information retrieval*, 13:291â€“314, 2010.

[^4]: Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.Piqa: Reasoning about physical commonsense in natural language.In *Association for the Advancement of Artificial Intelligence*, pages 7432â€“7439, 2020.

[^5]: Adam Casson.Transformer flops, 2023.[https://www.adamcasson.com/posts/transformer-flops](https://www.adamcasson.com/posts/transformer-flops).

[^6]: Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.Evaluating large language models trained on code, 2021.

[^7]: Dokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, and Noah Constant.Bridging the gap for tokenizer-free language models.*arXiv*, abs/1908.10322, 2019.

[^8]: Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.Hierarchical multiscale recurrent neural networks.In *Proceedings of the International Conference on Learning Representations*, 2019.

[^9]: Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting.Canine: Pre-training an efficient tokenization-free encoder for language representation.*Transactions of the Association for Computational Linguistics*, 10:73â€“91, 2022.

[^10]: Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.Think you have solved question answering? Try ARC, the AI2 reasoning challenge.*arXiv*, 2018.

[^11]: Gautier Dagan, Gabriel Synnaeve, and Baptiste Roziere.Getting the most out of your tokenizer for pre-training and domain adaptation.In *Forty-first International Conference on Machine Learning*, 2024.

[^12]: Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©.FlashAttention: Fast and memory-efficient exact attention with io-awareness.*Proceedings of Advances in Neural Information Processing Systems*, 35, 2022.

[^13]: Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.The llama 3 herd of models.*arXiv*, 2024.

[^14]: Lukas Edman, Helmut Schmid, and Alexander Fraser.CUTE: Measuring llmsâ€™ understanding of their tokens.*arXiv*, 2024.

[^15]: Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum, and Junâ€™ichi Tsujii.CharacterBERT: Reconciling elmo and bert for word-level open-vocabulary representations from characters.In *Proceedings of International Conference on Computational Linguistics*, 2020.

[^16]: Philip Gage.A new algorithm for data compression.*The C Users Journal*, 12(2):23â€“38, 1994.

[^17]: Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marcâ€™Aurelio Ranzato, Francisco GuzmÃ¡n, and Angela Fan.The Flores-101 evaluation benchmark for low-resource and multilingual machine translation.*Transactions of the Association for Computational Linguistics*, 10:522â€“538, 2022.[10.1162/tacl\_a\_00474](https://arxiv.org/doi.org/10.1162/tacl_a_00474).[https://aclanthology.org/2022.tacl-1.30](https://aclanthology.org/2022.tacl-1.30).

[^18]: Alex Graves.Generating sequences with recurrent neural networks.*arXiv*, 2013.

[^19]: Albert Gu and Tri Dao.Mamba: Linear-time sequence modeling with selective state spaces.*arXiv*, 2023.

[^20]: Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding.In *Proceedings of the International Conference on Learning Representations*, 2020.

[^21]: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.Training compute-optimal large language models.In *Proceedings of Advances in Neural Information Processing Systems*, 2022.

[^22]: Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.Perceiver: General perception with iterative attention.In *Proceedings of the International Conference of Machine Learning*. PMLR, 2021.

[^23]: Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, AÃ¤ron van den Oord, Alexander Graves, and Koray Kavukcuoglu.Neural machine translation in linear time.*arXiv*, 2016.

[^24]: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.Scaling laws for neural language models.*arXiv*, 2020.

[^25]: Tom Kenter, Llion Jones, and Daniel Hewlett.Byte-level machine reading across morphologically varied languages.In *Association for the Advancement of Artificial Intelligence*, 2018.

[^26]: Yoon Kim, Yacine Jernite, David Sontag, and Alexander Rush.Character-aware neural language models.In *Association for the Advancement of Artificial Intelligence*, 2016.

[^27]: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, and Noah Constant.Training llms over neurally compressed text.*arXiv*, 2024.

[^28]: Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al.Datacomp-lm: In search of the next generation of training sets for language models.*arXiv*, 2024.

[^29]: Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa.Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models.In *Proceedings of Empirical Methods in Natural Language Processing*, 2023.

[^30]: Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer.Myte: Morphology-driven byte encoding for better and fairer multilingual language modeling.*arXiv*, 2024.

[^31]: Ilya Loshchilov and Frank Hutter.Decoupled weight decay regularization.*arXiv*, 2017.

[^32]: TomÃ¡Å¡ Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernocky.Subword language modeling with neural networks.*preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf)*, 8(67), 2012.

[^33]: Piotr Nawrot, Szymon Tworkowski, MichaÅ‚ Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski.Hierarchical transformers are more efficient language models.In *Conference of the North American Chapter of the Association for Computational Linguistics*. Association for Computational Linguistics, 2022.

[^34]: Piotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo Maria Ponti.Efficient transformers with dynamic token pooling.In *Proceedings of the Association for Computational Linguistics*. Association for Computational Linguistics, 2023.

[^35]: Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi.Language model tokenizers introduce unfairness between languages.*Proceedings of Advances in Neural Information Processing Systems*, 2024.

[^36]: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners.*OpenAI blog*, 1(8):9, 2019.

[^37]: Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.In *Proceedings of the Association for Computational Linguistics*. Association for Computational Linguistics, 2016.

[^38]: Noam Shazeer.GLU variants improve transformer.*arXiv*, 2020.

[^39]: Kevin Slagle.Spacebyte: Towards deleting tokenization from large language modeling.*arXiv*, 2024.

[^40]: Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.RoFormer: Enhanced transformer with rotary position embedding. arxiv e-prints, art.*arXiv*, 2021.

[^41]: Ilya Sutskever, James Martens, and Geoffrey E Hinton.Generating text with recurrent neural networks.In *Proceedings of the International Conference of Machine Learning*, pages 1017â€“1024, 2011.

[^42]: Ashima Suvarna, Harshita Khandelwal, and Nanyun Peng.Phonologybench: Evaluating phonological skills of large language models.*arXiv*, 2024.

[^43]: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and fine-tuned chat models.*arXiv*, 2023.

[^44]: Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.Attention is all you need.In *Neural Information Processing Systems*, 2017.

[^45]: Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush.Mambabyte: Token-free selective state space model.*arXiv*, 2024.

[^46]: Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al.Effective long-context scaling of foundation models.In *Conference of the North American Chapter of the Association for Computational Linguistics*, 2024.

[^47]: Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel.Byt5: Towards a token-free future with pre-trained byte-to-byte models.*Transactions of the Association for Computational Linguistics*, 10:291â€“306, 2022.

[^48]: Lili Yu, DÃ¡niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.Megabyte: Predicting million-byte sequences with multiscale transformers.*Proceedings of Advances in Neural Information Processing Systems*, 2023.

[^49]: Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.Hellaswag: Can a machine really finish your sentence?*arXiv*, 2019.

[^50]: Biao Zhang and Rico Sennrich.Root mean square layer normalization.*Proceedings of Advances in Neural Information Processing Systems*, 32, 2019.

[^51]: Xiang Zhang, Junbo Zhao, and Yann LeCun.Character-level convolutional networks for text classification.In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, *Proceedings of Advances in Neural Information Processing Systems*, volume 28. Curran Associates, Inc., 2015.[https://proceedings.neurips.cc/paper\_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf).